import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import csv
import time
from collections import deque
from pysc2.env import sc2_env
from pysc2.lib import actions, features

# åŒ¯å…¥åº•å±¤è…³æœ¬
import production_ai 
from production_ai import ProductionAI

# =========================================================
# ğŸ’ çŒ´å­è£œä¸èˆ‡è·¯å¾‘è¨­å®š
# =========================================================
current_dir = os.path.dirname(os.path.abspath(__file__))
log_dir = os.path.join(current_dir, "log")

def patched_data_collector_init(self):
    if not os.path.exists(log_dir): os.makedirs(log_dir)
    self.filename = os.path.join(log_dir, f"terran_log_{int(time.time())}.csv")
    with open(self.filename, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["Game_Loop", "Minerals", "Vespene", "Workers", "Ideal", "Action_ID"])

production_ai.DataCollector.__init__ = patched_data_collector_init

# =========================================================
# ğŸ“Š è¨“ç·´ç´€éŒ„å™¨ (å·²ä¿®æ­£åƒæ•¸æ•¸é‡èˆ‡æ•´æ•¸è½‰æ›)
# =========================================================
class TrainingLogger:
    def __init__(self):
        if not os.path.exists(log_dir): os.makedirs(log_dir)
        self.filename = os.path.join(log_dir, f"dqn_training_log_{int(time.time())}.csv")
        with open(self.filename, mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(["Episode", "Epsilon", "Total_Reward", "Marauders", "End_Loop", "Reason", "Is_Bottom_Right"])

    def log_episode(self, ep, eps, reward, m_cnt, loop, reason, location):
        """ ç´€éŒ„æ¯å›åˆæ‘˜è¦ï¼ŒåŠ å…¥ eps åƒæ•¸ """
        if hasattr(reward, "item"): 
            reward = reward.item()
        int_reward = int(reward) 
        
        with open(self.filename, mode='a', newline='') as file:
            writer = csv.writer(file)
            # å¯«å…¥æ•¸æ“šæ™‚å°æ‡‰æ¨™é¡Œé †åº
            writer.writerow([ep, f"{eps:.3f}", int_reward, m_cnt, loop, reason, location])

# =========================================================
# ğŸ§  æ·±åº¦å­¸ç¿’æ¨¡å‹ (DQN)
# =========================================================
class QNetwork(nn.Module):
    def __init__(self, state_size, action_size, param_size=16):
        super(QNetwork, self).__init__()
        self.common = nn.Sequential(
            nn.Linear(state_size, 128), nn.ReLU(),
            nn.Linear(128, 64), nn.ReLU()
        )
        # å‹•ä½œé ­ï¼šæ±ºå®šåŸ·è¡Œå“ªå€‹ Action (0-9, 40)
        self.action_head = nn.Linear(64, action_size)
        # åƒæ•¸é ­ï¼šæ±ºå®šç›®æ¨™ç¶²æ ¼ (1-16)
        self.param_head = nn.Linear(64, param_size)

    def forward(self, x):
        x = self.common(x)
        return self.action_head(x), self.param_head(x) # åŒæ™‚å›å‚³å…©çµ„ Q å€¼
    
    def get_state_vector(obs, current_block):
        player = obs.observation.player
        m_unit = obs.observation.feature_minimap[features.MINIMAP_FEATURES.unit_type.index]
        s_unit = obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]
        
        # çµ±ä¸€è¨ˆç®—é‚è¼¯
        return [
            player.food_workers / 16,                               # 1. å·¥å…µ
            player.minerals / 1000,                                 # 2. ç¤¦çŸ³
            player.vespene / 500,                                  # 3. ç“¦æ–¯
            player.food_used / 50,                                 # 4. äººå£
            np.sum(m_unit == production_ai.BARRACKS_ID),            # 5. å…¨åœ°åœ–å…µç‡Ÿ (å°åœ°åœ–)
            np.sum(m_unit == production_ai.REFINERY_ID),            # 6. å…¨åœ°åœ–ç“¦æ–¯å»  (å°åœ°åœ–)
            np.sum(m_unit == production_ai.BARRACKS_TECHLAB_ID),    # 7. å…¨åœ°åœ–å¯¦é©—å®¤ (å°åœ°åœ–)
            int(np.sum(s_unit == 51) / 20) / 10,                   # 8. æ å¥ªè€… (ç•«é¢)
            current_block / 16.0,                                   # 9. è¦–è§’ä½ç½®
            float(np.sum(s_unit == 21) > 0),                        # 10. ç•«é¢æ˜¯å¦æœ‰å…µç‡Ÿ (è½‰ç‚º 0.0/1.0)
            1.0                                                     # 11. å¸¸æ•¸
        ]

# =========================================================
# ğŸ® è¨“ç·´ä¸»ç¨‹å¼
# =========================================================
def main(argv):
    del argv
    state_size = 11 # å¢åŠ ä¸€æ ¼ç‹€æ…‹ç´€éŒ„ã€Œç›®å‰çœ‹å“ªã€
    action_size = 41
    
    brain_model = QNetwork(state_size, action_size)
    optimizer = optim.Adam(brain_model.parameters(), lr=0.0005) 
    criterion = nn.MSELoss()
    memory = deque(maxlen=100000) 
    logger = TrainingLogger()
    learn_min = 0.01 # é€™æ˜¯ä½ çš„ epsilon æœ€å°å€¼
    
    model_path = os.path.join(log_dir, "dqn_model.pth")
    if os.path.exists(model_path):
        brain_model.load_state_dict(torch.load(model_path))
        print("âœ… è¼‰å…¥æˆåŠŸï¼æ¥çºŒä¹‹å‰çš„è¨˜æ†¶ç¹¼çºŒè¨“ç·´...")

    epsilon = 1.0; epsilon_decay = 0.999; gamma = 0.99 

    with sc2_env.SC2Env(
        map_name="Simple64",
        players=[sc2_env.Agent(sc2_env.Race.terran), sc2_env.Agent(sc2_env.Race.terran)],
        agent_interface_format=sc2_env.AgentInterfaceFormat(
            feature_dimensions=sc2_env.Dimensions(screen=84, minimap=64), use_raw_units=False),
        step_mul=16, realtime=False
    ) as env:
        for ep in range(100):
            hands = ProductionAI() 
            print(f"\nğŸš€ === å•Ÿå‹•ç¬¬ {ep+1} å›åˆ (Epsilon: {epsilon:.3f}) ===")
            obs_list = env.reset()
            last_m=0; last_b=0; last_r=0; last_t=0
            total_reward = 0
            
            while True:
                obs = obs_list[0]
                player = obs.observation.player # æ–°å¢ï¼šæå– player è³‡è¨Š
                current_workers = player.food_workers # 
                minimap_unit_type = obs.observation.feature_minimap[features.MINIMAP_FEATURES.unit_type.index]
                
                # --- ã€ä¿®æ­£ 1ã€‘ æå–å¿…è¦è®Šæ•¸ ---
                unit_type = obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]
                r_cnt = np.sum(unit_type == 20)  
                b_cnt = np.sum(unit_type == 21)  
                t_cnt = np.sum(unit_type == 37)  
                m_cnt = int(np.sum(unit_type == 51) / 20) 
                curr_loop = obs.observation.game_loop[0]
                # åœ¨å°åœ°åœ–ä¸­ï¼Œå»ºç¯‰ç‰©ä¹Ÿæœƒä»¥å°æ‡‰çš„ ID é¡¯ç¤º
                global_b_cnt = np.sum(minimap_unit_type == production_ai.BARRACKS_ID)
                global_r_cnt = np.sum(minimap_unit_type == production_ai.REFINERY_ID)
                global_t_cnt = np.sum(minimap_unit_type == production_ai.BARRACKS_TECHLAB_ID)
                
                # åµæ¸¬ç•¶å‰ç•«é¢ (Screen) æ˜¯å¦æœ‰å»ºç¯‰ï¼Œé€™èƒ½å¹«åŠ© AI å­¸ç¿’ã€Œç§»å‹•è¦–è§’ã€çš„å¿…è¦æ€§
                screen_unit = obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]
                screen_b_cnt = np.sum(screen_unit == production_ai.BARRACKS_ID)

                # æå–ç‹€æ…‹ (ç¢ºä¿ current_workers èˆ‡ player å·²å®šç¾©)
                current_block = getattr(hands, 'active_parameter', 1)
                state = [
                    current_workers / 16,
                    player.minerals / 1000,
                    player.vespene / 500,
                    player.food_used / 50,
                    global_b_cnt,             # å…¨åœ°åœ–å…µç‡Ÿç¸½æ•¸
                    global_r_cnt,             # å…¨åœ°åœ–ç“¦æ–¯å» ç¸½æ•¸
                    global_t_cnt,             # å…¨åœ°åœ–å¯¦é©—å®¤ç¸½æ•¸
                    m_cnt / 10.0,             
                    current_block / 16.0,     # ç›®å‰çœ‹å“ªè£¡
                    screen_b_cnt > 0,         # ç›®å‰ç•«é¢çœ‹å¾—åˆ°å…µç‡Ÿå—ï¼Ÿ (å¸ƒæ—å€¼è½‰ 0/1)
                    1.0
                ]
                state_t = torch.FloatTensor(np.array(state))

                # 2. åŒæ™‚é¸æ“‡å‹•ä½œèˆ‡åƒæ•¸ (Epsilon-Greedy)
                if random.random() <= epsilon:
                    a_id = random.randint(1, 40)# å¾å¯ç”¨å‹•ä½œä¸­é¸
                    p_id = random.randint(1, 16) # éš¨æ©Ÿé¸ä¸€å€‹ç¶²æ ¼
                else:
                    with torch.no_grad():
                        q_actions, q_params = brain_model(state_t.unsqueeze(0))
                        a_id = torch.argmax(q_actions).item()
                        p_id = torch.argmax(q_params).item() + 1

                # 3. åŸ·è¡Œå‹•ä½œï¼šå‚³å…¥åƒæ•¸
                sc2_action = hands.get_action(obs, a_id, parameter=p_id)
                obs_list = env.step([sc2_action, actions.FUNCTIONS.no_op()])
                
                # 4. çå‹µé‚è¼¯ (ç¶­æŒåŸæœ‰æ¶æ§‹)
                # --- çå‹µé‚è¼¯ (å¼·åŒ–æ å¥ªè€…æ¬Šé‡) ---
                step_reward = -0.01 
                if m_cnt > last_m:
                    step_reward += 200.0  
                    last_m = m_cnt
                    if m_cnt >= 5: 
                        step_reward += 1000.0

                total_reward += step_reward


                # 5. æå–ä¸‹ä¸€å€‹ç‹€æ…‹ä¸¦å­˜å…¥è¨˜æ†¶
                next_obs = obs_list[0]
                next_unit = next_obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]
                # --- ä¿®æ­£å¾Œçš„ next_state (ç¢ºä¿èˆ‡ state çš„ 11 ç¶­åº¦å®Œå…¨å°é½Š) ---
                next_player = next_obs.observation.player
                next_unit = next_obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]

                next_state = [
                    next_player.food_workers / 16,               # 1. å·¥å…µ (åŸä»£ç¢¼æ¼æ‰é€™é …å°è‡´ç¶­åº¦è®Š 10)
                    next_player.minerals / 1000,                # 2. ç¤¦çŸ³
                    next_player.vespene / 500,                 # 3. ç“¦æ–¯
                    next_player.food_used / 50,                 # 4. äººå£
                    np.sum(next_unit == 21),                    # 5. å…µç‡Ÿ
                    np.sum(next_unit == 20),                    # 6. ç“¦æ–¯å» 
                    np.sum(next_unit == 37),                    # 7. ç§‘æŠ€å¯¦é©—å®¤
                    int(np.sum(next_unit == 51) / 20) / 10,     # 8. æ å¥ªè€…
                    current_block / 16.0,                       # 9. ç›®å‰è¦–è§’
                    0,                                          # 10. é ç•™
                    1.0                                         # 11. å¸¸æ•¸
                ]
                
                done = bool(next_obs.last() or m_cnt >= 5 or curr_loop >= 13440)
                memory.append((state, int(a_id), int(p_id), float(step_reward), next_state, bool(done)))

                # --- ğŸ§  æ¨¡å‹å­¸ç¿’éƒ¨åˆ†çš„ä¿®æ­£ ---
                # --- å­¸ç¿’éƒ¨åˆ†çš„é›™é ­ Loss ---
                if len(memory) > 256:
                    batch = random.sample(memory, 64)
                    # åŠ å…¥ b_params
                    b_states, b_actions, b_params, b_rewards, b_next_states, b_dones = zip(*batch)
                    
                    b_states_t = torch.as_tensor(np.array(b_states), dtype=torch.float32)
                    b_next_states_t = torch.as_tensor(np.array(b_next_states), dtype=torch.float32)
                    b_actions_t = torch.as_tensor(b_actions, dtype=torch.long)
                    b_params_t = torch.as_tensor(b_params, dtype=torch.long) - 1 # è½‰å› 0-15 ç´¢å¼•
                    b_rewards_t = torch.as_tensor(b_rewards, dtype=torch.float32)
                    b_dones_t = torch.as_tensor(np.array(b_dones, dtype=np.float32))

                    # åŒæ™‚è¨ˆç®—ç•¶å‰å‹•ä½œèˆ‡åƒæ•¸çš„ Q å€¼
                    curr_q_a, curr_q_p = brain_model(b_states_t)
                    # åŒæ™‚è¨ˆç®—ä¸‹ä¸€å€‹ç‹€æ…‹çš„å‹•ä½œèˆ‡åƒæ•¸ Q å€¼
                    next_q_a, next_q_p = brain_model(b_next_states_t)
                    
                    # å‹•ä½œ Loss è¨ˆç®—
                    targets_a = b_rewards_t + (1 - b_dones_t) * gamma * next_q_a.max(1)[0].detach()
                    loss_a = criterion(curr_q_a.gather(1, b_actions_t.unsqueeze(1)).squeeze(1), targets_a)
                    
                    # åƒæ•¸ Loss è¨ˆç®— (è®“ç¶²æ ¼é¸æ“‡ä¹Ÿè·Ÿè‘—çå‹µå­¸ç¿’)
                    targets_p = b_rewards_t + (1 - b_dones_t) * gamma * next_q_p.max(1)[0].detach()
                    loss_p = criterion(curr_q_p.gather(1, b_params_t.unsqueeze(1)).squeeze(1), targets_p)
                    
                    # åˆä½µ Loss ä¸¦æ›´æ–°æ¨¡å‹
                    total_loss = loss_a + loss_p
                    optimizer.zero_grad()
                    total_loss.backward()
                    optimizer.step()

                if done:
                    loc_text = (production_ai.BASE_LOCATION_CODE == 1)
                    reason = "Target_Reached" if m_cnt >= 5 else "Timeout"
                    
                    logger.log_episode(ep+1, epsilon, total_reward, m_cnt, curr_loop, reason, loc_text)
                    
                    # ã€ä¿®æ­£ã€‘å°‡ worker_cnt æ”¹ç‚º current_workers
                    print(f"å›åˆçµæŸ | æ å¥ªè€…æ•¸é‡: {m_cnt} | å·¥å…µæ•¸é‡: {current_workers} | ç¸½åˆ†: {int(total_reward)}")
                    break
            
            # å›åˆçµæŸå¾Œæ›´æ–° epsilon
            epsilon = max(learn_min, epsilon * epsilon_decay)
            torch.save(brain_model.state_dict(), model_path)

if __name__ == "__main__":
    from absl import app
    app.run(main)