import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import csv
import time
from collections import deque
from pysc2.env import sc2_env
from pysc2.lib import actions, features

# åŒ¯å…¥åº•å±¤è…³æœ¬
import production_ai 
from production_ai import ProductionAI
import logging
from absl import logging as absl_logging

# å±è”½ features.py ç”¢å‡ºçš„è­¦å‘Šè¨Šæ¯
absl_logging.set_verbosity(absl_logging.ERROR)
# --- 1. å®šç¾© Action ID èˆ‡ Unit ID çš„å°æ‡‰  ---
TARGET_UNIT_MAP = {
    14: production_ai.SCV_ID,       16: 48,  # Marine
    17: 49,  # Reaper              18: production_ai.MARAUDER_ID,
    19: 50,  # Ghost               20: 53,  # Hellion
    21: 484, # Hellbat             22: 498, # WidowMine
    23: 33,  # SiegeTank           24: 692, # Cyclone
    25: 52,  # Thor                26: 34,  # Viking
    27: 54,  # Medivac             28: 689, # Liberator
    29: 56,  # Raven               30: 57,  # Battlecruiser
    31: 55,  # Banshee             32: production_ai.PLANETARY_FORTRESS_ID
}

PIXELS_PER_UNIT = {
    production_ai.SCV_ID: 15,
    48: 10,  # Marine
    49: 15,  # Reaper
    production_ai.MARAUDER_ID: 22, # Marauder é«”å‹è¼ƒå¤§ï¼Œç´„ 20-25 åƒç´ 
    50: 15,  # Ghost
    33: 150, # Siege Tank (å»ºç¯‰/é‡å‹å–®ä½åƒç´ è¼ƒå¤š)
    # å»ºç¯‰ç‰©é¡å»ºè­°åªè¦åƒç´  > 0 å°±ç®— 1 æ£Ÿï¼Œæˆ–æ˜¯çµ¦äºˆè¼ƒå¤§é™¤æ•¸
}
# =========================================================
# ğŸ’ è·¯å¾‘è¨­å®š
# =========================================================
current_dir = os.path.dirname(os.path.abspath(__file__))
log_dir = os.path.join(current_dir, "log")

def patched_data_collector_init(self):
    if not os.path.exists(log_dir): os.makedirs(log_dir)
    self.filename = os.path.join(log_dir, f"terran_log_{int(time.time())}.csv")
    with open(self.filename, mode='w', newline='') as file:
        writer = csv.writer(file)
        # ã€åŒæ­¥ã€‘åŠ å…¥ Barracks
        writer.writerow(["Game_Loop", "Minerals", "Vespene", "Workers", "Ideal", "Barracks", "Action_ID"])

production_ai.DataCollector.__init__ = patched_data_collector_init

# =========================================================
# ğŸ“Š è¨“ç·´ç´€éŒ„å™¨ (å·²ä¿®æ­£åƒæ•¸æ•¸é‡èˆ‡æ•´æ•¸è½‰æ›)
# =========================================================
class TrainingLogger:
    def __init__(self):
        if not os.path.exists(log_dir): os.makedirs(log_dir)
        self.filename = os.path.join(log_dir, f"dqn_training_log_{int(time.time())}.csv")
        with open(self.filename, mode='w', newline='') as file:
            writer = csv.writer(file)
            # ã€åŒæ­¥æ¨™é ­ã€‘ç¢ºä¿åŒ…å«æ‰€æœ‰çµ±è¨ˆé …ç›®
            writer.writerow(["Episode", "Epsilon", "Total_Reward", "Barracks", "TechLabs", "Marauders", "End_Loop", "Reason", "Is_Bottom_Right"])

    # ã€ä¿®æ­£å®šç¾©ã€‘å¢åŠ  t_cnt åƒæ•¸ï¼Œä½¿å…¶ç¸½å…±æ¥æ”¶ 9 å€‹åƒæ•¸ (å« self)
    def log_episode(self, ep, eps, reward, b_cnt, t_cnt, m_cnt, loop, reason, location):
        """ ç´€éŒ„æ¯å›åˆæ‘˜è¦ï¼Œç¢ºä¿èˆ‡å‚³å…¥åƒæ•¸æ•¸é‡ä¸€è‡´ """
        if hasattr(reward, "item"): 
            reward = reward.item()
        
        with open(self.filename, mode='a', newline='') as file:
            writer = csv.writer(file)
            # ä¾åºå¯«å…¥æ•¸æ“š
            writer.writerow([ep, f"{eps:.3f}", int(reward), b_cnt, t_cnt, m_cnt, loop, reason, location])

# =========================================================
# ğŸ§  æ·±åº¦å­¸ç¿’æ¨¡å‹ (DQN)
# =========================================================
class QNetwork(nn.Module):
    def __init__(self, state_size, action_size, param_size=16):
        super(QNetwork, self).__init__()
        self.common = nn.Sequential(
            nn.Linear(state_size, 128), nn.ReLU(),
            nn.Linear(128, 64), nn.ReLU()
        )
        # å‹•ä½œé ­ï¼šæ±ºå®šåŸ·è¡Œå“ªå€‹ Action (0-9, 40)
        self.action_head = nn.Linear(64, action_size)
        # åƒæ•¸é ­ï¼šæ±ºå®šç›®æ¨™ç¶²æ ¼ (1-16)
        self.param_head = nn.Linear(64, param_size)

    def forward(self, x):
        x = self.common(x)
        return self.action_head(x), self.param_head(x) # åŒæ™‚å›å‚³å…©çµ„ Q å€¼
    
def get_state_vector(obs, current_block, target_project_id):
    player = obs.observation.player
    m_unit = obs.observation.feature_minimap[features.MINIMAP_FEATURES.unit_type.index]
    m_relative = obs.observation.feature_minimap[features.MINIMAP_FEATURES.player_relative.index]
    
    # åµæ¸¬é¸å–ç‹€æ…‹
    is_scv_selected = 0.0
    is_cc_selected = 0.0
    if len(obs.observation.single_select) > 0:
        u_type = obs.observation.single_select[0].unit_type
        if u_type == production_ai.SCV_ID: is_scv_selected = 1.0
        if u_type == production_ai.COMMAND_CENTER_ID: is_cc_selected = 1.0

    # ç¢ºä¿å›å‚³ 12 å€‹ç‰¹å¾µ
    return [
        player.food_workers / 16,
        player.minerals / 1000,
        player.vespene / 500,
        player.food_used / 50,
        np.sum((m_unit == production_ai.BARRACKS_ID) & (m_relative == 1)),
        np.sum((m_unit == production_ai.SUPPLY_DEPOT_ID) & (m_relative == 1)),
        np.sum((m_unit == production_ai.REFINERY_ID) & (m_relative == 1)),
        np.sum((m_unit == production_ai.BARRACKS_TECHLAB_ID) & (m_relative == 1)),
        current_block / 16.0,
        is_scv_selected, 
        is_cc_selected,
        target_project_id / 40.0
    ]
    

# =========================================================
# ğŸ® è¨“ç·´ä¸»ç¨‹å¼
# =========================================================
def main(argv):
    del argv
    state_size = 12 # å¢åŠ ä¸€æ ¼ç‹€æ…‹ç´€éŒ„ã€Œç›®å‰çœ‹å“ªã€
    action_size = 43
    train_step_counter = 0
    CURRENT_TRAIN_TASK = 18
    brain_model = QNetwork(state_size, action_size)
    optimizer = optim.Adam(brain_model.parameters(), lr=0.0005) 
    criterion = nn.MSELoss()
    memory = deque(maxlen=100000) 
    logger = TrainingLogger()
    learn_min = 0.01 # é€™æ˜¯ä½ çš„ epsilon æœ€å°å€¼
    
    model_path = os.path.join(log_dir, "dqn_model.pth")
    if os.path.exists(model_path):
        brain_model.load_state_dict(torch.load(model_path))
        print("âœ… è¼‰å…¥æˆåŠŸï¼æ¥çºŒä¹‹å‰çš„è¨˜æ†¶ç¹¼çºŒè¨“ç·´...")

    epsilon = 1.0; epsilon_decay = 0.999; gamma = 0.99 

    with sc2_env.SC2Env(
        map_name="Simple96",
        players=[sc2_env.Agent(sc2_env.Race.terran), sc2_env.Agent(sc2_env.Race.terran)],
        agent_interface_format=sc2_env.AgentInterfaceFormat(
            feature_dimensions=sc2_env.Dimensions(screen=84, minimap=64), use_raw_units=False),
        step_mul=16, realtime=False
    ) as env:
        for ep in range(1000):
            # --- 1. åˆå§‹åŒ–ç’°å¢ƒèˆ‡è®Šæ•¸ (ä¿®å¾© UnboundLocalError) ---
            hands = ProductionAI() 
            obs_list = env.reset() 
            obs = obs_list[0]  # ã€é—œéµã€‘ç¢ºä¿é€²å…¥ while ä¹‹å‰ obs å·²è¢«å®šç¾©
            
            # åˆå§‹åŒ–è¿½è¹¤è®Šæ•¸
            last_target_count = 0 
            rewarded_depots = 0     # ã€æ–°å¢ã€‘ç´€éŒ„å·²çµ¦åˆ†éçš„è£œçµ¦ç«™æ•¸é‡
            last_d_pixels = 0
            scv_reward_count = 0
            has_rewarded_barracks = False 
            has_rewarded_techlab = False  
            has_rewarded_home = False # ã€æ–°å¢ã€‘ä¸€æ¬¡æ€§å›å®¶çå‹µæ——æ¨™
            has_rewarded_control_group = False
            total_reward = 0.0
            # é è¨­å‹•ä½œèˆ‡åƒæ•¸
            a_id = 40; p_id = 1 

            while True:
                # --- 1. å–å¾—ç•¶å‰ç‹€æ…‹èˆ‡é¸æ“‡å‹•ä½œ ---
                current_block = getattr(hands, 'active_parameter', 1)
                state = get_state_vector(obs, current_block, CURRENT_TRAIN_TASK)
                state_t = torch.FloatTensor(np.array(state))

                # Epsilon-Greedy é¸æ“‡ (a_id æ±ºå®šåšä»€éº¼ï¼Œp_id æ±ºå®šåœ¨å“ªåš)
                if random.random() <= epsilon:
                    a_id = random.randint(1, 41) 
                    p_id = random.randint(1, 16)
                else:
                    with torch.no_grad():
                        q_actions, q_params = brain_model(state_t.unsqueeze(0))
                        a_id = torch.argmax(q_actions).item()
                        p_id = torch.argmax(q_params).item() + 1


                # --- 2. åŸ·è¡Œå–®ä¸€å‹•ä½œ (ç§»é™¤åŸæœ‰çš„è‡ªå‹•åˆ‡æ›è¦–è§’é‚è¼¯) ---
                # --- 2. åŸ·è¡Œå–®ä¸€å‹•ä½œ ---
                # --- 2. åŸ·è¡Œå–®ä¸€å‹•ä½œ ---
                sc2_action = hands.get_action(obs, a_id, parameter=p_id)
                actual_id = hands.locked_action if hands.locked_action is not None else a_id
                
                # --- 2. åŸ·è¡Œå‹•ä½œèˆ‡å–å¾—æ–°è§€å¯Ÿ ---
                obs_list = env.step([sc2_action, actions.FUNCTIONS.no_op()])
                next_obs = obs_list[0]
                
                # --- ã€é—œéµä¿®æ­£ï¼šè®Šæ•¸å®šç¾©å¿…é ˆç§»åˆ°æœ€å‰é¢ã€‘ ---
                obs_data = next_obs.observation 
                next_m_unit = obs_data.feature_minimap[features.MINIMAP_FEATURES.unit_type.index]
                next_m_relative = obs_data.feature_minimap[features.MINIMAP_FEATURES.player_relative.index]
                next_s_unit = obs_data.feature_screen[features.SCREEN_FEATURES.unit_type.index]
                next_s_relative = obs_data.feature_screen[features.SCREEN_FEATURES.player_relative.index]

                # --- 3. æ•¸æ“šè¨ˆç®—èˆ‡ç´€éŒ„ (ç¾åœ¨è®Šæ•¸å·²å®šç¾©ï¼Œä¸æœƒå ±éŒ¯äº†) ---
                curr_b_count = np.sum((next_m_unit == production_ai.BARRACKS_ID) & (next_m_relative == 1))
                
                # å‘¼å«ç´€éŒ„å™¨å°‡æ¯ä¸€æ­¥å¯«å…¥ terran_log
                hands.collector.log_step(
                    obs_data.game_loop,        # Time
                    obs_data.player.minerals, 
                    obs_data.player.vespene,
                    obs_data.player.food_workers, 
                    16, curr_b_count, actual_id
                )

                # --- 4. çå‹µåˆ¤å®šèˆ‡å°å‡ºè³‡è¨Š ---
                step_reward = -0.01 
                if train_step_counter % 10 == 0:
                    print(f"Episode {ep+1} | åŸ·è¡Œå‹•ä½œ: {a_id} | åƒæ•¸: {p_id} | ç¤¦çŸ³: {obs_data.player.minerals}")
                # ã€ä¿®æ­£ã€‘è¨ˆç®—æ å¥ªè€…æ•¸é‡ (åŸæœ¬ä»£ç¢¼æ¼æ‰é€™æ®µï¼Œæœƒå°è‡´ NameError)
                self_m_pixels = np.sum((next_s_unit == production_ai.MARAUDER_ID) & (next_s_relative == 1))
                real_m_count = int(np.round(float(self_m_pixels) / 22.0))

                # åµæ¸¬é¸å–ç‹€æ…‹
                is_scv_selected = False
                is_cc_selected = False
                if len(obs_data.single_select) > 0:
                    u_type = obs_data.single_select[0].unit_type
                    if u_type == production_ai.SCV_ID: is_scv_selected = True
                    if u_type == production_ai.COMMAND_CENTER_ID: is_cc_selected = True
                elif len(obs_data.multi_select) > 0:
                    if any(u.unit_type == production_ai.SCV_ID for u in obs_data.multi_select):
                        is_scv_selected = True

                # --- 4. çå‹µé‚è¼¯ï¼šå¼•å° AI ä¸»å‹•åˆ‡æ›è¦–è§’ ---
                
                # A. å·¥å…µé¸å–å¼•å° (é™å‰ 50 æ¬¡)
                # A. å·¥å…µé¸å–çå‹µ (é™å‰ 50 æ¬¡)
                # --- 4. çå‹µé‚è¼¯å„ªåŒ– ---
                # å¦‚æœæ­£è™•æ–¼é–å®šå»ºç¯‰å‹•ä½œä¸­ (hands.locked_action) ä¸” æ²’é¸ä¸­å·¥å…µ
                # E. æ å¥ªè€…ç”¢å‡ºçš„ã€Œéšæ¢¯å¼ã€çå‹µ
                # --- æ å¥ªè€…ç”¢å‡ºçå‹µï¼šç°¡å–®æ¸…æ™°çš„çµ¦åˆ† ---
                if real_m_count > last_target_count:
                    new_units = real_m_count - last_target_count
                    # æ¯ç”¢å‡ºä¸€éš»å°±çµ¦ 300 åˆ†ï¼Œé€™èƒ½è®“ AI æ˜ç™½ç”¢å…µæ¯”è·³è¦–è§’é‡è¦ 30 å€
                    step_reward += (new_units * 300.0)
                    print(f"ğŸ¯ æˆåŠŸç”¢å‡º {new_units} éš»æ å¥ªè€…ï¼ç´¯ç©çå‹µ +{new_units * 300.0}")
                    
                    # é”æˆ 5 éš»å°±çµ¦ä¸€å€‹è¶…å¤§çµ‚çµçé‡‘
                    if real_m_count >= 5:
                        step_reward += 1000.0
                    
                    last_target_count = real_m_count

                # ã€ä¿®æ­£ã€‘åˆªé™¤åŸæœ¬ä»£ç¢¼ä¸­é‡è¤‡çš„ total_reward += step_reward
                total_reward += step_reward

                # --- 5. ç‹€æ…‹æ›´æ–°èˆ‡å­˜å…¥è¨˜æ†¶ ---
                updated_block = getattr(hands, 'active_parameter', 1)
                next_state = get_state_vector(next_obs, updated_block, CURRENT_TRAIN_TASK)
                # ç¾åœ¨ real_m_count å·²ç¶“å®šç¾©ï¼Œä¸æœƒå†å ±éŒ¯
                done = bool(next_obs.last() or real_m_count >= 5 or next_obs.observation.game_loop[0] >= 20160)
                # ç¢ºä¿å­˜å…¥è¨˜æ†¶çš„æ˜¯ ProductionAI çœŸæ­£åŸ·è¡Œçš„é‚£å€‹å‹•ä½œ ID
                # å¦‚æœé€™ä¸€æ­¥å› ç‚ºé–å®šæ©Ÿåˆ¶åŸ·è¡Œäº† Action 1ï¼Œå³ä¾¿éš¨æ©ŸæŠ½åˆ° 40ï¼Œä¹Ÿè¦è¨˜ç‚º 1
                actual_action_id = hands.locked_action if hands.locked_action is not None else a_id
                memory.append((state, int(actual_action_id), int(p_id), float(step_reward), next_state, bool(done)))
                obs = next_obs
                # --- 6. æ¨¡å‹è¨“ç·´ (æ‰¹æ¬¡å­¸ç¿’) ---
                # --- 6. æ¨¡å‹è¨“ç·´ (æ‰¹æ¬¡å­¸ç¿’) ---
                train_step_counter += 1
                if len(memory) > 1000 and train_step_counter % 8 == 0:
                    batch = random.sample(memory, 64)
                    
                    # æº–å‚™æ‰¹æ¬¡æ•¸æ“š
                    states, actions_id, params_id, rewards, next_states, dones = zip(*batch)
                    
                    states_t = torch.FloatTensor(np.array(states))
                    next_states_t = torch.FloatTensor(np.array(next_states))
                    actions_t = torch.LongTensor(actions_id)
                    params_t = torch.LongTensor(params_id) - 1 # è½‰å› 0-15 ç´¢å¼•
                    rewards_t = torch.FloatTensor(rewards)
                    dones_t = torch.FloatTensor(dones)

                    # è¨ˆç®—ç•¶å‰ Q å€¼
                    current_q_actions, current_q_params = brain_model(states_t)
                    q_a = current_q_actions.gather(1, actions_t.unsqueeze(1)).squeeze(1)
                    q_p = current_q_params.gather(1, params_t.unsqueeze(1)).squeeze(1)

                    # è¨ˆç®—ç›®æ¨™ Q å€¼ (Double DQN ç°¡åŒ–ç‰ˆ)
                    with torch.no_grad():
                        next_q_actions, next_q_params = brain_model(next_states_t)
                        max_next_q_a = next_q_actions.max(1)[0]
                        max_next_q_p = next_q_params.max(1)[0]
                        target_a = rewards_t + (1 - dones_t) * gamma * max_next_q_a
                        target_p = rewards_t + (1 - dones_t) * gamma * max_next_q_p

                    # ç®—æå¤±ä¸¦æ›´æ–°
                    loss = criterion(q_a, target_a) + criterion(q_p, target_p)
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                if done:
                    # çµ±è¨ˆå…µç‡Ÿèˆ‡ç§‘æŠ€å¯¦é©—å®¤ (å…¨åŸŸæƒæ)
                    final_b_pixels = np.sum((next_m_unit == production_ai.BARRACKS_ID) & (next_m_relative == 1))
                    final_b_count = 1 if final_b_pixels > 0 else 0
                    
                    final_t_pixels = np.sum((next_m_unit == production_ai.BARRACKS_TECHLAB_ID) & (next_m_relative == 1))
                    final_t_count = 1 if final_t_pixels > 0 else 0
                    
                    # ã€æ ¸å¿ƒä¿®æ­£ã€‘é€™è£¡å‚³å…¥çš„åƒæ•¸é †åºå¿…é ˆèˆ‡ log_episode å®šç¾©ä¸€è‡´
                    logger.log_episode(
                        ep + 1,            # Episode (ç¬¬å¹¾æ¬¡)
                        epsilon,           # Epsilon
                        total_reward,      # ç¸½åˆ†
                        final_b_count,     # å…µç‡Ÿ
                        final_t_count,     # ç§‘æŠ€å¯¦é©—å®¤
                        real_m_count,      # æ å¥ªè€… (ç‹©çµè€…)
                        next_obs.observation.game_loop[0], # Loop
                        "Done",            # Reason
                        (production_ai.BASE_LOCATION_CODE == 1) # Location
                    )
                    
                    # æ§åˆ¶å°åŒæ­¥è¼¸å‡ºçµ±è¨ˆå…§å®¹
                    print(f"\n" + "="*40)
                    print(f"ğŸ ç¬¬ {ep+1} æ¬¡ è¨“ç·´çµç®—")
                    print(f"ğŸ  å…µç‡Ÿ: {final_b_count} | ğŸ§ª å¯¦é©—å®¤: {final_t_count} | ğŸ¯ æ å¥ªè€…: {real_m_count}")
                    print(f"ğŸ’° ç¸½åˆ†: {int(total_reward)}")
                    print("="*40 + "\n")
                    break
            
            # å›åˆçµæŸå¾Œæ›´æ–° epsilon
            epsilon = max(learn_min, epsilon * epsilon_decay)
            torch.save(brain_model.state_dict(), model_path)

if __name__ == "__main__":
    from absl import app
    app.run(main)