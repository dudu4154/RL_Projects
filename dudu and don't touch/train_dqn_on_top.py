import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import csv
import time
from collections import deque
from pysc2.env import sc2_env
from pysc2.lib import actions, features

# åŒ¯å…¥åº•å±¤è…³æœ¬
import production_ai 
from production_ai import ProductionAI
import logging
from absl import logging as absl_logging

# å±è”½ features.py ç”¢å‡ºçš„è­¦å‘Šè¨Šæ¯
absl_logging.set_verbosity(absl_logging.ERROR)
# --- 1. å®šç¾© Action ID èˆ‡ Unit ID çš„å°æ‡‰  ---
TARGET_UNIT_MAP = {
    14: production_ai.SCV_ID,       16: 48,  # Marine
    17: 49,  # Reaper              18: production_ai.MARAUDER_ID,
    19: 50,  # Ghost               20: 53,  # Hellion
    21: 484, # Hellbat             22: 498, # WidowMine
    23: 33,  # SiegeTank           24: 692, # Cyclone
    25: 52,  # Thor                26: 34,  # Viking
    27: 54,  # Medivac             28: 689, # Liberator
    29: 56,  # Raven               30: 57,  # Battlecruiser
    31: 55,  # Banshee             32: production_ai.PLANETARY_FORTRESS_ID
}

PIXELS_PER_UNIT = {
    production_ai.SCV_ID: 15,
    48: 10,  # Marine
    49: 15,  # Reaper
    production_ai.MARAUDER_ID: 22, # Marauder é«”å‹è¼ƒå¤§ï¼Œç´„ 20-25 åƒç´ 
    50: 15,  # Ghost
    33: 150, # Siege Tank (å»ºç¯‰/é‡å‹å–®ä½åƒç´ è¼ƒå¤š)
    # å»ºç¯‰ç‰©é¡å»ºè­°åªè¦åƒç´  > 0 å°±ç®— 1 æ£Ÿï¼Œæˆ–æ˜¯çµ¦äºˆè¼ƒå¤§é™¤æ•¸
}
# =========================================================
# ğŸ’ è·¯å¾‘è¨­å®š
# =========================================================
current_dir = os.path.dirname(os.path.abspath(__file__))
log_dir = os.path.join(current_dir, "log")

def patched_data_collector_init(self):
    if not os.path.exists(log_dir): os.makedirs(log_dir)
    self.filename = os.path.join(log_dir, f"terran_log_{int(time.time())}.csv")
    with open(self.filename, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["Game_Loop", "Minerals", "Vespene", "Workers", "Ideal", "Action_ID"])

production_ai.DataCollector.__init__ = patched_data_collector_init

# =========================================================
# ğŸ“Š è¨“ç·´ç´€éŒ„å™¨ (å·²ä¿®æ­£åƒæ•¸æ•¸é‡èˆ‡æ•´æ•¸è½‰æ›)
# =========================================================
class TrainingLogger:
    def __init__(self):
        if not os.path.exists(log_dir): os.makedirs(log_dir)
        self.filename = os.path.join(log_dir, f"dqn_training_log_{int(time.time())}.csv")
        with open(self.filename, mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(["Episode", "Epsilon", "Total_Reward", "Marauders", "End_Loop", "Reason", "Is_Bottom_Right"])

    def log_episode(self, ep, eps, reward, m_cnt, loop, reason, location):
        """ ç´€éŒ„æ¯å›åˆæ‘˜è¦ï¼ŒåŠ å…¥ eps åƒæ•¸ """
        if hasattr(reward, "item"): 
            reward = reward.item()
        int_reward = int(reward) 
        
        with open(self.filename, mode='a', newline='') as file:
            writer = csv.writer(file)
            # å¯«å…¥æ•¸æ“šæ™‚å°æ‡‰æ¨™é¡Œé †åº
            writer.writerow([ep, f"{eps:.3f}", int_reward, m_cnt, loop, reason, location])

# =========================================================
# ğŸ§  æ·±åº¦å­¸ç¿’æ¨¡å‹ (DQN)
# =========================================================
class QNetwork(nn.Module):
    def __init__(self, state_size, action_size, param_size=16):
        super(QNetwork, self).__init__()
        self.common = nn.Sequential(
            nn.Linear(state_size, 128), nn.ReLU(),
            nn.Linear(128, 64), nn.ReLU()
        )
        # å‹•ä½œé ­ï¼šæ±ºå®šåŸ·è¡Œå“ªå€‹ Action (0-9, 40)
        self.action_head = nn.Linear(64, action_size)
        # åƒæ•¸é ­ï¼šæ±ºå®šç›®æ¨™ç¶²æ ¼ (1-16)
        self.param_head = nn.Linear(64, param_size)

    def forward(self, x):
        x = self.common(x)
        return self.action_head(x), self.param_head(x) # åŒæ™‚å›å‚³å…©çµ„ Q å€¼
    
def get_state_vector(obs, current_block, target_project_id):
    player = obs.observation.player
    m_unit = obs.observation.feature_minimap[features.MINIMAP_FEATURES.unit_type.index]
    s_unit = obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]
    # ã€æ–°å¢ã€‘å–å¾—è¢å¹•æ­¸å±¬å±¤ (1: è‡ªå·±, 3: ä¸­ç«‹, 4: æ•µäºº)
    s_relative = obs.observation.feature_screen[features.SCREEN_FEATURES.player_relative.index]
    
    target_unit_id = TARGET_UNIT_MAP.get(target_project_id, 0)
    
    # ã€æ ¸å¿ƒä¿®æ­£ã€‘éæ¿¾èƒŒæ™¯ï¼Œåªç®—è‡ªå·±çš„å–®ä½
    if target_project_id == 14: 
        current_target_count = float(player.food_workers)
    else:
        # åªè¨ˆç®— (ID ç¬¦åˆ) ä¸” (å±¬æ–¼è‡ªå·±) çš„åƒç´ 
        self_pixels = np.sum((s_unit == target_unit_id) & (s_relative == 1))
        # æ›ç®—ç‚ºå–®ä½æ•¸
        divisor = PIXELS_PER_UNIT.get(target_unit_id, 20)
        current_target_count = float(self_pixels) / float(divisor)

    return [
        player.food_workers / 16,
        player.minerals / 1000,
        player.vespene / 500,
        player.food_used / 50,
        np.sum(m_unit == production_ai.BARRACKS_ID),
        np.sum(m_unit == production_ai.REFINERY_ID),
        np.sum(m_unit == production_ai.BARRACKS_TECHLAB_ID),
        current_target_count / 10.0,
        current_block / 16.0,
        float(np.sum((s_unit == production_ai.BARRACKS_ID) & (s_relative == 1)) > 0), # å…µç‡Ÿä¹ŸåŒæ­¥éæ¿¾
        1.0,
        target_project_id / 40.0
    ]

# =========================================================
# ğŸ® è¨“ç·´ä¸»ç¨‹å¼
# =========================================================
def main(argv):
    del argv
    state_size = 12 # å¢åŠ ä¸€æ ¼ç‹€æ…‹ç´€éŒ„ã€Œç›®å‰çœ‹å“ªã€
    action_size = 41
    CURRENT_TRAIN_TASK = 18
    brain_model = QNetwork(state_size, action_size)
    optimizer = optim.Adam(brain_model.parameters(), lr=0.0005) 
    criterion = nn.MSELoss()
    memory = deque(maxlen=100000) 
    logger = TrainingLogger()
    learn_min = 0.01 # é€™æ˜¯ä½ çš„ epsilon æœ€å°å€¼
    
    model_path = os.path.join(log_dir, "dqn_model.pth")
    if os.path.exists(model_path):
        brain_model.load_state_dict(torch.load(model_path))
        print("âœ… è¼‰å…¥æˆåŠŸï¼æ¥çºŒä¹‹å‰çš„è¨˜æ†¶ç¹¼çºŒè¨“ç·´...")

    epsilon = 1.0; epsilon_decay = 0.999; gamma = 0.99 

    with sc2_env.SC2Env(
        map_name="Simple64",
        players=[sc2_env.Agent(sc2_env.Race.terran), sc2_env.Agent(sc2_env.Race.terran)],
        agent_interface_format=sc2_env.AgentInterfaceFormat(
            feature_dimensions=sc2_env.Dimensions(screen=84, minimap=64), use_raw_units=False),
        step_mul=16, realtime=False
    ) as env:
        for ep in range(100):
            hands = ProductionAI() 
            print(f"\nğŸš€ === å•Ÿå‹•ç¬¬ {ep+1} å›åˆ (Epsilon: {epsilon:.3f}) ===")
            obs_list = env.reset()
            last_m=0; last_b=0; last_r=0; last_t=0
            total_reward = 0
            last_target_count = 0
            

            while True:
                obs = obs_list[0]
                player = obs.observation.player # æ–°å¢ï¼šæå– player è³‡è¨Š
                current_workers = player.food_workers # 
                current_block = getattr(hands, 'active_parameter', 1)
                minimap_unit_type = obs.observation.feature_minimap[features.MINIMAP_FEATURES.unit_type.index]
                state = get_state_vector(obs, current_block, CURRENT_TRAIN_TASK)
                # --- ã€ä¿®æ­£ 1ã€‘ æå–å¿…è¦è®Šæ•¸ ---
                unit_type = obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]
                r_cnt = np.sum(unit_type == 20)  
                b_cnt = np.sum(unit_type == 21)  
                t_cnt = np.sum(unit_type == 37)  
                m_cnt = int(np.sum(unit_type == 51) / 20) 
                curr_loop = obs.observation.game_loop[0]
                # åœ¨å°åœ°åœ–ä¸­ï¼Œå»ºç¯‰ç‰©ä¹Ÿæœƒä»¥å°æ‡‰çš„ ID é¡¯ç¤º
                global_b_cnt = np.sum(minimap_unit_type == production_ai.BARRACKS_ID)
                global_r_cnt = np.sum(minimap_unit_type == production_ai.REFINERY_ID)
                global_t_cnt = np.sum(minimap_unit_type == production_ai.BARRACKS_TECHLAB_ID)
                
                # åµæ¸¬ç•¶å‰ç•«é¢ (Screen) æ˜¯å¦æœ‰å»ºç¯‰ï¼Œé€™èƒ½å¹«åŠ© AI å­¸ç¿’ã€Œç§»å‹•è¦–è§’ã€çš„å¿…è¦æ€§
                screen_unit = obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]
                screen_b_cnt = np.sum(screen_unit == production_ai.BARRACKS_ID)

                # æå–ç‹€æ…‹ (ç¢ºä¿ current_workers èˆ‡ player å·²å®šç¾©)
                current_block = getattr(hands, 'active_parameter', 1)
                state = get_state_vector(obs, current_block, CURRENT_TRAIN_TASK)
                # --- 1. é¸æ“‡å‹•ä½œ ---
                state_t = torch.FloatTensor(np.array(state))
                if random.random() <= epsilon:
                    a_id = random.randint(1, 40)
                    p_id = random.randint(1, 16)
                else:
                    with torch.no_grad():
                        q_actions, q_params = brain_model(state_t.unsqueeze(0))
                        a_id = torch.argmax(q_actions).item()
                        p_id = torch.argmax(q_params).item() + 1

                # --- 2. åŸ·è¡Œå‹•ä½œ (åªåŸ·è¡Œä¸€æ¬¡ env.step) ---
                sc2_action = hands.get_action(obs, a_id, parameter=p_id)
                obs_list = env.step([sc2_action, actions.FUNCTIONS.no_op()])
                next_obs = obs_list[0]
                
                # --- 3. çå‹µé‚è¼¯è¨ˆç®— ---
                # --- 3. çå‹µé‚è¼¯è¨ˆç®— (çµ±ä¸€ç‰ˆ) ---
                step_reward = -0.01  # åŸºç¤æ™‚é–“æ‡²ç½°
                
                next_s_unit = next_obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]
                next_s_relative = next_obs.observation.feature_screen[features.SCREEN_FEATURES.player_relative.index]
                target_uid = TARGET_UNIT_MAP.get(CURRENT_TRAIN_TASK, 0)
                
                if CURRENT_TRAIN_TASK == 14:
                    curr_count = float(next_obs.observation.player.food_workers)
                else:
                    # è¨ˆç®—ã€Œå±¬æ–¼æˆ‘ã€çš„ç›®æ¨™åƒç´ 
                    self_pixels = np.sum((next_s_unit == target_uid) & (next_s_relative == 1))
                    divisor = PIXELS_PER_UNIT.get(target_uid, 20)
                    # é€™è£¡æ˜¯æµ®é»æ•¸é™¤æ³•ï¼Œä¸æœƒå ± CastingError
                    curr_count = float(self_pixels) / float(divisor)

                # åªè¦ã€Œå–®ä½æ•¸é‡ã€å¢åŠ ï¼Œå°±çµ¦äºˆçå‹µ
                # ä½¿ç”¨ round è™•ç†å¾®å°åƒç´ æ³¢å‹•
                if round(curr_count) > round(last_target_count):
                    reward_value = 200.0
                    step_reward += reward_value
                    print(f"ğŸ¯ è¨“ç·´æˆåŠŸ! é …ç›®:{CURRENT_TRAIN_TASK} | å–®ä½æ•¸:{int(round(curr_count))} | çå‹µ +{reward_value}")
                    last_target_count = curr_count

                total_reward += step_reward

                # 5. æå–ä¸‹ä¸€å€‹ç‹€æ…‹ä¸¦å­˜å…¥è¨˜æ†¶
                next_obs = obs_list[0]
                next_unit = next_obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]
                # --- ä¿®æ­£å¾Œçš„ next_state (ç¢ºä¿èˆ‡ state çš„ 11 ç¶­åº¦å®Œå…¨å°é½Š) ---
                next_player = next_obs.observation.player
                next_unit = next_obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]

                updated_block = getattr(hands, 'active_parameter', 1)
                next_state = get_state_vector(next_obs, updated_block, CURRENT_TRAIN_TASK)
                
                # åˆ¤æ–·æ˜¯å¦çµæŸ
                m_cnt_now = int(np.sum(next_s_unit == 51) / 20) 
                done = bool(next_obs.last() or m_cnt_now >= 5 or next_obs.observation.game_loop[0] >= 13440)
                
                memory.append((state, int(a_id), int(p_id), float(step_reward), next_state, bool(done)))

                # --- ğŸ§  æ¨¡å‹å­¸ç¿’éƒ¨åˆ†çš„ä¿®æ­£ ---
                # --- å­¸ç¿’éƒ¨åˆ†çš„é›™é ­ Loss ---
                if len(memory) > 256:
                    batch = random.sample(memory, 64)
                    # åŠ å…¥ b_params
                    b_states, b_actions, b_params, b_rewards, b_next_states, b_dones = zip(*batch)
                    
                    b_states_t = torch.as_tensor(np.array(b_states), dtype=torch.float32)
                    b_next_states_t = torch.as_tensor(np.array(b_next_states), dtype=torch.float32)
                    b_actions_t = torch.as_tensor(b_actions, dtype=torch.long)
                    b_params_t = torch.as_tensor(b_params, dtype=torch.long) - 1 # è½‰å› 0-15 ç´¢å¼•
                    b_rewards_t = torch.as_tensor(b_rewards, dtype=torch.float32)
                    b_dones_t = torch.as_tensor(np.array(b_dones, dtype=np.float32))

                    # åŒæ™‚è¨ˆç®—ç•¶å‰å‹•ä½œèˆ‡åƒæ•¸çš„ Q å€¼
                    curr_q_a, curr_q_p = brain_model(b_states_t)
                    # åŒæ™‚è¨ˆç®—ä¸‹ä¸€å€‹ç‹€æ…‹çš„å‹•ä½œèˆ‡åƒæ•¸ Q å€¼
                    next_q_a, next_q_p = brain_model(b_next_states_t)
                    
                    # å‹•ä½œ Loss è¨ˆç®—
                    targets_a = b_rewards_t + (1 - b_dones_t) * gamma * next_q_a.max(1)[0].detach()
                    loss_a = criterion(curr_q_a.gather(1, b_actions_t.unsqueeze(1)).squeeze(1), targets_a)
                    
                    # åƒæ•¸ Loss è¨ˆç®— (è®“ç¶²æ ¼é¸æ“‡ä¹Ÿè·Ÿè‘—çå‹µå­¸ç¿’)
                    targets_p = b_rewards_t + (1 - b_dones_t) * gamma * next_q_p.max(1)[0].detach()
                    loss_p = criterion(curr_q_p.gather(1, b_params_t.unsqueeze(1)).squeeze(1), targets_p)
                    
                    # åˆä½µ Loss ä¸¦æ›´æ–°æ¨¡å‹
                    total_loss = loss_a + loss_p
                    optimizer.zero_grad()
                    total_loss.backward()
                    optimizer.step()

                if done:
                    loc_text = (production_ai.BASE_LOCATION_CODE == 1)
                    reason = "Target_Reached" if m_cnt >= 5 else "Timeout"
                    
                    logger.log_episode(ep+1, epsilon, total_reward, m_cnt, curr_loop, reason, loc_text)
                    
                    # ã€ä¿®æ­£ã€‘å°‡ worker_cnt æ”¹ç‚º current_workers
                    print(f"å›åˆçµæŸ | æ å¥ªè€…æ•¸é‡: {m_cnt} | å·¥å…µæ•¸é‡: {current_workers} | ç¸½åˆ†: {int(total_reward)}")
                    break
            
            # å›åˆçµæŸå¾Œæ›´æ–° epsilon
            epsilon = max(learn_min, epsilon * epsilon_decay)
            torch.save(brain_model.state_dict(), model_path)

if __name__ == "__main__":
    from absl import app
    app.run(main)