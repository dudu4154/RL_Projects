import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import csv
import time
from collections import deque
from pysc2.env import sc2_env
from pysc2.lib import actions, features

# åŒ¯å…¥åº•å±¤è…³æœ¬
import production_ai 
from production_ai import ProductionAI

# =========================================================
# ğŸ’ çŒ´å­è£œä¸èˆ‡è·¯å¾‘è¨­å®š
# =========================================================
current_dir = os.path.dirname(os.path.abspath(__file__))
log_dir = os.path.join(current_dir, "log")

def patched_data_collector_init(self):
    if not os.path.exists(log_dir): os.makedirs(log_dir)
    self.filename = os.path.join(log_dir, f"terran_log_{int(time.time())}.csv")
    with open(self.filename, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["Game_Loop", "Minerals", "Vespene", "Workers", "Ideal", "Action_ID"])

production_ai.DataCollector.__init__ = patched_data_collector_init

# =========================================================
# ğŸ“Š è¨“ç·´ç´€éŒ„å™¨ (å·²ä¿®æ­£åƒæ•¸æ•¸é‡èˆ‡æ•´æ•¸è½‰æ›)
# =========================================================
class TrainingLogger:
    def __init__(self):
        if not os.path.exists(log_dir): os.makedirs(log_dir)
        self.filename = os.path.join(log_dir, f"dqn_training_log_{int(time.time())}.csv")
        with open(self.filename, mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(["Episode", "Total_Reward", "Marauders", "End_Loop", "Reason", "Is_Bottom_Right"])

    def log_episode(self, ep, reward, m_cnt, loop, reason, location):
        """ ç´€éŒ„æ¯å›åˆæ‘˜è¦ï¼Œå°‡çå‹µè½‰ç‚ºæ•´æ•¸ """
        # ç¢ºä¿çå‹µæ˜¯ç´”æ•¸å­—ä¸¦è½‰æ›ç‚ºæ•´æ•¸
        if hasattr(reward, "item"): 
            reward = reward.item()
        int_reward = int(reward) 
        
        with open(self.filename, mode='a', newline='') as file:
            writer = csv.writer(file)
            # ä½¿ç”¨èˆ‡å‚³å…¥åƒæ•¸ä¸€è‡´çš„è®Šæ•¸åç¨±
            writer.writerow([ep, int_reward, m_cnt, loop, reason, location])
            
# =========================================================
# ğŸ§  æ·±åº¦å­¸ç¿’æ¨¡å‹ (DQN)
# =========================================================
class QNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(QNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_size, 128), nn.ReLU(),
            nn.Linear(128, 64), nn.ReLU(),
            nn.Linear(64, action_size)
        )
    def forward(self, x): return self.fc(x)

# =========================================================
# ğŸ® è¨“ç·´ä¸»ç¨‹å¼
# =========================================================
def main(argv):
    del argv
    state_size = 10; action_size = 10
    
    brain_model = QNetwork(state_size, action_size)
    optimizer = optim.Adam(brain_model.parameters(), lr=0.0005) 
    criterion = nn.MSELoss()
    memory = deque(maxlen=10000) 
    logger = TrainingLogger()
    
    model_path = os.path.join(log_dir, "dqn_model.pth")
    if os.path.exists(model_path):
        brain_model.load_state_dict(torch.load(model_path))
        print("âœ… è¼‰å…¥æˆåŠŸï¼æ¥çºŒä¹‹å‰çš„è¨˜æ†¶ç¹¼çºŒè¨“ç·´...")

    epsilon = 1.0; epsilon_decay = 0.995; gamma = 0.99 

    with sc2_env.SC2Env(
        map_name="Simple64",
        players=[sc2_env.Agent(sc2_env.Race.terran), sc2_env.Agent(sc2_env.Race.terran)],
        agent_interface_format=sc2_env.AgentInterfaceFormat(
            feature_dimensions=sc2_env.Dimensions(screen=84, minimap=64), use_raw_units=False),
        step_mul=32, realtime=False
    ) as env:
        for ep in range(200):
            hands = ProductionAI() 
            print(f"\nğŸš€ === å•Ÿå‹•ç¬¬ {ep+1} å›åˆ (Epsilon: {epsilon:.3f}) ===")
            obs_list = env.reset()
            last_m=0; last_b=0; last_r=0; last_t=0
            total_reward = 0
            
            while True:
                obs = obs_list[0]
                curr_loop = int(obs.observation.game_loop)
                
                # 1. æå–ç‹€æ…‹
                unit_type = obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]
                b_cnt = np.sum(unit_type == 21); r_cnt = np.sum(unit_type == 20)
                t_cnt = np.sum(unit_type == 37); m_cnt = int(np.sum(unit_type == 51) / 20)
                
                state = [
                    obs.observation.player.minerals / 1000, obs.observation.player.vespene / 500,
                    obs.observation.player.food_used / 50, b_cnt, r_cnt, t_cnt, m_cnt / 10,
                    0, 0, 1.0
                ]
                # å„ªåŒ–ï¼šå…ˆè½‰ numpy é™£åˆ—å†è½‰ Tensor
                state_t = torch.FloatTensor(np.array(state))

                # 2. é¸æ“‡å‹•ä½œ
                if random.random() <= epsilon:
                    a_id = random.randint(0, 9)
                else:
                    with torch.no_grad(): a_id = torch.argmax(brain_model(state_t.unsqueeze(0))).item()

                # 3. åŸ·è¡Œå‹•ä½œ
                sc2_action = hands.get_action(obs, a_id)
                obs_list = env.step([sc2_action, actions.FUNCTIONS.no_op()])
                
                # 4. çå‹µé‚è¼¯ (ç¶­æŒåŸæœ‰æ¶æ§‹)
                step_reward = -0.01 
                if r_cnt > last_r and r_cnt <= 2: step_reward += 15.0; last_r = r_cnt
                if b_cnt > last_b and b_cnt <= 2: step_reward += 20.0; last_b = b_cnt
                if t_cnt > last_t: step_reward += 40.0; last_t = t_cnt
                if m_cnt > last_m:
                    step_reward += 150.0; last_m = m_cnt
                    if m_cnt >= 5: step_reward += 500.0
                total_reward += step_reward

                # 5. æå–ä¸‹ä¸€å€‹ç‹€æ…‹ä¸¦å­˜å…¥è¨˜æ†¶
                next_obs = obs_list[0]
                next_unit = next_obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]
                next_state = [
                    next_obs.observation.player.minerals / 1000, next_obs.observation.player.vespene / 500,
                    next_obs.observation.player.food_used / 50, np.sum(next_unit==21), 
                    np.sum(next_unit==20), np.sum(next_unit==37), int(np.sum(next_unit==51)/20) / 10,
                    0, 0, 1.0
                ]
                
                done = bool(next_obs.last() or m_cnt >= 5 or curr_loop >= 13440)
                # å­˜å…¥è¨˜æ†¶æ™‚å¼·åˆ¶è½‰æ›é¡å‹
                memory.append((state, int(a_id), float(step_reward), next_state, bool(done)))

                # --- ğŸ§  æ¨¡å‹å­¸ç¿’éƒ¨åˆ†çš„ä¿®æ­£ ---
                if len(memory) > 128:
                    batch = random.sample(memory, 64)
                    b_states, b_actions, b_rewards, b_next_states, b_dones = zip(*batch)
                    
                    # ä½¿ç”¨ torch.as_tensor æˆ–å…ˆè½‰ç‚º float é¡å‹çš„ numpy é™£åˆ—
                    b_states_t = torch.as_tensor(np.array(b_states), dtype=torch.float32)
                    b_next_states_t = torch.as_tensor(np.array(b_next_states), dtype=torch.float32)
                    b_actions_t = torch.as_tensor(b_actions, dtype=torch.long)
                    b_rewards_t = torch.as_tensor(b_rewards, dtype=torch.float32)
                    
                    # é€™è£¡æœ€é—œéµï¼šå…ˆè½‰æˆ float çš„ numpy é™£åˆ—ï¼Œå†è½‰ Tensor
                    b_dones_t = torch.as_tensor(np.array(b_dones, dtype=np.float32))

                    with torch.no_grad():
                        # ä½¿ç”¨ .max(1)[0] ç¢ºä¿ Q å€¼ç¶­åº¦æ­£ç¢º
                        next_q = brain_model(b_next_states_t).max(1)[0]
                        targets = b_rewards_t + (1 - b_dones_t) * gamma * next_q
                    
                    current_q = brain_model(b_states_t).gather(1, b_actions_t.unsqueeze(1)).squeeze(1)
                    loss = criterion(current_q, targets)
                    optimizer.zero_grad(); loss.backward(); optimizer.step()

                if done:
                    # è®€å–å‡ºç”Ÿé»ç‹€æ…‹
                    loc_text = (production_ai.BASE_LOCATION_CODE == 1)
                    reason = "Target_Reached" if m_cnt >= 5 else "Timeout"
                    
                    # å‚³å…¥ 6 å€‹åƒæ•¸çµ¦ç´€éŒ„å™¨
                    logger.log_episode(ep+1, total_reward, m_cnt, curr_loop, reason, loc_text)
                    
                    # çµ‚ç«¯æ©Ÿé¡¯ç¤ºåŒæ¨£è½‰ç‚ºæ•´æ•¸
                    print(f"å›åˆçµæŸ | å‡ºç”Ÿé»å³ä¸‹: {loc_text} ({production_ai.BASE_LOCATION_CODE}) | "
                        f"ç”¢é‡: {int(m_cnt)} | ç¸½åˆ†: {int(total_reward)}")
                    break
            
            epsilon = max(0.99, epsilon * epsilon_decay)
            torch.save(brain_model.state_dict(), model_path)

if __name__ == "__main__":
    from absl import app
    app.run(main)