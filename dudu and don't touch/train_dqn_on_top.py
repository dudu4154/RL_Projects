import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import csv
import time
from collections import deque
from pysc2.env import sc2_env
from pysc2.lib import actions, features

# åŒ¯å…¥åº•å±¤è…³æœ¬
import production_ai 
from production_ai import ProductionAI
import logging
from absl import logging as absl_logging

# å±è”½ features.py ç”¢å‡ºçš„è­¦å‘Šè¨Šæ¯
absl_logging.set_verbosity(absl_logging.ERROR)
# --- 1. å®šç¾© Action ID èˆ‡ Unit ID çš„å°æ‡‰  ---
TARGET_UNIT_MAP = {
    14: production_ai.SCV_ID,       16: 48,  # Marine
    17: 49,  # Reaper              18: production_ai.MARAUDER_ID,
    19: 50,  # Ghost               20: 53,  # Hellion
    21: 484, # Hellbat             22: 498, # WidowMine
    23: 33,  # SiegeTank           24: 692, # Cyclone
    25: 52,  # Thor                26: 34,  # Viking
    27: 54,  # Medivac             28: 689, # Liberator
    29: 56,  # Raven               30: 57,  # Battlecruiser
    31: 55,  # Banshee             32: production_ai.PLANETARY_FORTRESS_ID
}

PIXELS_PER_UNIT = {
    production_ai.SCV_ID: 15,
    48: 10,  # Marine
    49: 15,  # Reaper
    production_ai.MARAUDER_ID: 22, # Marauder é«”å‹è¼ƒå¤§ï¼Œç´„ 20-25 åƒç´ 
    50: 15,  # Ghost
    33: 150, # Siege Tank (å»ºç¯‰/é‡å‹å–®ä½åƒç´ è¼ƒå¤š)
    # å»ºç¯‰ç‰©é¡å»ºè­°åªè¦åƒç´  > 0 å°±ç®— 1 æ£Ÿï¼Œæˆ–æ˜¯çµ¦äºˆè¼ƒå¤§é™¤æ•¸
}
# =========================================================
# ğŸ’ è·¯å¾‘è¨­å®š
# =========================================================
current_dir = os.path.dirname(os.path.abspath(__file__))
log_dir = os.path.join(current_dir, "log")

def patched_data_collector_init(self):
    if not os.path.exists(log_dir): os.makedirs(log_dir)
    self.filename = os.path.join(log_dir, f"terran_log_{int(time.time())}.csv")
    with open(self.filename, mode='w', newline='') as file:
        writer = csv.writer(file)
        # ã€åŒæ­¥ã€‘åŠ å…¥ Barracks
        writer.writerow(["Game_Loop", "Minerals", "Vespene", "Workers", "Ideal", "Barracks", "Action_ID"])

production_ai.DataCollector.__init__ = patched_data_collector_init

# =========================================================
# ğŸ“Š è¨“ç·´ç´€éŒ„å™¨ (å·²ä¿®æ­£åƒæ•¸æ•¸é‡èˆ‡æ•´æ•¸è½‰æ›)
# =========================================================
class TrainingLogger:
    def __init__(self):
        if not os.path.exists(log_dir): os.makedirs(log_dir)
        self.filename = os.path.join(log_dir, f"dqn_training_log_{int(time.time())}.csv")
        with open(self.filename, mode='w', newline='') as file:
            writer = csv.writer(file)
            # ã€åŒæ­¥æ¨™é ­ã€‘ç¢ºä¿åŒ…å«æ‰€æœ‰çµ±è¨ˆé …ç›®
            writer.writerow(["Episode", "Epsilon", "Total_Reward", "Barracks", "TechLabs", "Marauders", "End_Loop", "Reason", "Is_Bottom_Right"])

    # ã€ä¿®æ­£å®šç¾©ã€‘å¢åŠ  t_cnt åƒæ•¸ï¼Œä½¿å…¶ç¸½å…±æ¥æ”¶ 9 å€‹åƒæ•¸ (å« self)
    def log_episode(self, ep, eps, reward, b_cnt, t_cnt, m_cnt, loop, reason, location):
        """ ç´€éŒ„æ¯å›åˆæ‘˜è¦ï¼Œç¢ºä¿èˆ‡å‚³å…¥åƒæ•¸æ•¸é‡ä¸€è‡´ """
        if hasattr(reward, "item"): 
            reward = reward.item()
        
        with open(self.filename, mode='a', newline='') as file:
            writer = csv.writer(file)
            # ä¾åºå¯«å…¥æ•¸æ“š
            writer.writerow([ep, f"{eps:.3f}", int(reward), b_cnt, t_cnt, m_cnt, loop, reason, location])

# =========================================================
# ğŸ§  æ·±åº¦å­¸ç¿’æ¨¡å‹ (DQN)
# =========================================================
class QNetwork(nn.Module):
    def __init__(self, state_size, action_size, param_size=16):
        super(QNetwork, self).__init__()
        self.common = nn.Sequential(
            nn.Linear(state_size, 128), nn.ReLU(),
            nn.Linear(128, 64), nn.ReLU()
        )
        # å‹•ä½œé ­ï¼šæ±ºå®šåŸ·è¡Œå“ªå€‹ Action (0-9, 40)
        self.action_head = nn.Linear(64, action_size)
        # åƒæ•¸é ­ï¼šæ±ºå®šç›®æ¨™ç¶²æ ¼ (1-16)
        self.param_head = nn.Linear(64, param_size)

    def forward(self, x):
        x = self.common(x)
        return self.action_head(x), self.param_head(x) # åŒæ™‚å›å‚³å…©çµ„ Q å€¼
    
def get_state_vector(obs, current_block, target_project_id):
    player = obs.observation.player
    m_unit = obs.observation.feature_minimap[features.MINIMAP_FEATURES.unit_type.index]
    m_relative = obs.observation.feature_minimap[features.MINIMAP_FEATURES.player_relative.index]
    
    # åµæ¸¬é¸å–ç‹€æ…‹
    is_scv_selected = 0.0
    is_cc_selected = 0.0
    if len(obs.observation.single_select) > 0:
        u_type = obs.observation.single_select[0].unit_type
        if u_type == production_ai.SCV_ID: is_scv_selected = 1.0
        if u_type == production_ai.COMMAND_CENTER_ID: is_cc_selected = 1.0

    # ç¢ºä¿å›å‚³ 12 å€‹ç‰¹å¾µ
    return [
        player.food_workers / 16,
        player.minerals / 1000,
        player.vespene / 500,
        player.food_used / 50,
        np.sum((m_unit == production_ai.BARRACKS_ID) & (m_relative == 1)),
        np.sum((m_unit == production_ai.SUPPLY_DEPOT_ID) & (m_relative == 1)),
        np.sum((m_unit == production_ai.REFINERY_ID) & (m_relative == 1)),
        np.sum((m_unit == production_ai.BARRACKS_TECHLAB_ID) & (m_relative == 1)),
        current_block / 16.0,
        is_scv_selected, 
        is_cc_selected,
        target_project_id / 40.0
    ]
    

# =========================================================
# ğŸ® è¨“ç·´ä¸»ç¨‹å¼
# =========================================================
def main(argv):
    del argv
    state_size = 12 # å¢åŠ ä¸€æ ¼ç‹€æ…‹ç´€éŒ„ã€Œç›®å‰çœ‹å“ªã€
    action_size = 43
    CURRENT_TRAIN_TASK = 18
    brain_model = QNetwork(state_size, action_size)
    optimizer = optim.Adam(brain_model.parameters(), lr=0.0005) 
    criterion = nn.MSELoss()
    memory = deque(maxlen=100000) 
    logger = TrainingLogger()
    learn_min = 0.01 # é€™æ˜¯ä½ çš„ epsilon æœ€å°å€¼
    
    model_path = os.path.join(log_dir, "dqn_model.pth")
    if os.path.exists(model_path):
        brain_model.load_state_dict(torch.load(model_path))
        print("âœ… è¼‰å…¥æˆåŠŸï¼æ¥çºŒä¹‹å‰çš„è¨˜æ†¶ç¹¼çºŒè¨“ç·´...")

    epsilon = 1.0; epsilon_decay = 0.999; gamma = 0.99 

    with sc2_env.SC2Env(
        map_name="Simple64",
        players=[sc2_env.Agent(sc2_env.Race.terran), sc2_env.Agent(sc2_env.Race.terran)],
        agent_interface_format=sc2_env.AgentInterfaceFormat(
            feature_dimensions=sc2_env.Dimensions(screen=84, minimap=64), use_raw_units=False),
        step_mul=16, realtime=False
    ) as env:
        for ep in range(1000):
            # --- 1. åˆå§‹åŒ–ç’°å¢ƒèˆ‡è®Šæ•¸ (ä¿®å¾© UnboundLocalError) ---
            hands = ProductionAI() 
            obs_list = env.reset() 
            obs = obs_list[0]  # ã€é—œéµã€‘ç¢ºä¿é€²å…¥ while ä¹‹å‰ obs å·²è¢«å®šç¾©
            
            # åˆå§‹åŒ–è¿½è¹¤è®Šæ•¸
            last_target_count = 0 
            rewarded_depots = 0     # ã€æ–°å¢ã€‘ç´€éŒ„å·²çµ¦åˆ†éçš„è£œçµ¦ç«™æ•¸é‡
            last_d_pixels = 0
            has_rewarded_barracks = False 
            has_rewarded_techlab = False  
            has_rewarded_home = False # ã€æ–°å¢ã€‘ä¸€æ¬¡æ€§å›å®¶çå‹µæ——æ¨™
            has_rewarded_control_group = False
            total_reward = 00
            # é è¨­å‹•ä½œèˆ‡åƒæ•¸
            a_id = 40; p_id = 1 

            while True:
                # --- 1. å–å¾—ç•¶å‰ç‹€æ…‹èˆ‡é¸æ“‡å‹•ä½œ (è£œå…¨è¢«çœç•¥çš„éƒ¨åˆ†) ---
                current_block = getattr(hands, 'active_parameter', 1)
                state = get_state_vector(obs, current_block, CURRENT_TRAIN_TASK)
                state_t = torch.FloatTensor(np.array(state))

                # Epsilon-Greedy é¸æ“‡å‹•ä½œ
                if random.random() <= epsilon:
                    a_id = random.randint(1, 42)
                    p_id = random.randint(1, 16)
                else:
                    with torch.no_grad():
                        q_actions, q_params = brain_model(state_t.unsqueeze(0))
                        a_id = torch.argmax(q_actions).item()
                        p_id = torch.argmax(q_params).item() + 1

                # --- 2. åŸ·è¡Œå‹•ä½œ ---
                sc2_action = hands.get_action(obs, a_id, parameter=p_id)
                obs_list = env.step([sc2_action, actions.FUNCTIONS.no_op()])
                next_obs = obs_list[0]
                
                # --- 4. çå‹µé‚è¼¯ä¿®æ­£ ---
                step_reward = -0.01 
                
                # ã€æ ¸å¿ƒä¿®æ­£ã€‘åœ¨çå‹µåˆ¤å®šå‰å®šç¾©è®Šæ•¸
                obs_data = next_obs.observation
                is_scv_selected = False
                is_cc_selected = False
                
                # å–å¾—æœ€æ–°ä¸€å¹€çš„ç‰¹å¾µ
                next_s_unit = next_obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]
                next_s_relative = next_obs.observation.feature_screen[features.SCREEN_FEATURES.player_relative.index]
                next_m_unit = next_obs.observation.feature_minimap[features.MINIMAP_FEATURES.unit_type.index]
                next_m_relative = next_obs.observation.feature_minimap[features.MINIMAP_FEATURES.player_relative.index]

                # A. ã€æ–°å¢ã€‘è£œçµ¦ç«™çå‹µ (é™å‰ 2 å€‹ï¼Œä½¿ç”¨å…¨åŸŸå°åœ°åœ–åˆ¤å®š)
                curr_d_pixels = np.sum((next_m_unit == production_ai.SUPPLY_DEPOT_ID) & (next_m_relative == 1))
                if curr_d_pixels > last_d_pixels:
                    if rewarded_depots < 2:
                        rewarded_depots += 1
                        step_reward += 30.0 # è£œçµ¦ç«™æ¬Šé‡æé«˜è‡³ 30
                        print(f"ğŸ  åµæ¸¬åˆ°æ–°è£œçµ¦ç«™å®Œå·¥ï¼ç´¯è¨ˆ: {rewarded_depots} | çå‹µ +30")
                    last_d_pixels = curr_d_pixels

                if is_scv_selected:
                    step_reward += 0.5 
                if is_cc_selected:
                    step_reward += 0.5
                
                if 1 <= a_id <= 13 and not is_scv_selected:
                    step_reward -= 0.1
                # B. ã€ä¿®æ­£ã€‘å›å®¶çå‹µ (æ¯å±€é™ä¸€æ¬¡)
                if a_id == 40 and not has_rewarded_home:
                    cc_visible = np.any((next_s_unit == production_ai.COMMAND_CENTER_ID) & (next_s_relative == 1))
                    if cc_visible:
                        step_reward += 10.0 
                        has_rewarded_home = True
                        print(f"ğŸ  ç¬¬ä¸€æ¬¡æ‰¾åˆ°åŸºåœ°ï¼çå‹µ +10")

                # C. ã€ä¿®æ­£ã€‘Action 41 ç·¨éšŠçå‹µ (æ”¹ç”¨ next_obs åˆ¤å®šçµæœ)
                if a_id == 41 and not has_rewarded_control_group:
                    # å¿…é ˆæª¢æŸ¥å‹•ä½œåŸ·è¡Œã€Œå¾Œã€çš„çµæœ
                    is_cc_selected_now = False
                    obs_data = next_obs.observation # ä½¿ç”¨ä¸‹ä¸€æ­¥çš„è³‡æ–™
                    
                if len(obs_data.single_select) > 0:
                    u_type = obs_data.single_select[0].unit_type
                    if u_type == production_ai.SCV_ID: is_scv_selected = True
                    if u_type == production_ai.COMMAND_CENTER_ID: is_cc_selected = True
                elif len(obs_data.multi_select) > 0:
                    if obs_data.multi_select[0].unit_type == production_ai.SCV_ID: is_scv_selected = True
                    
                    # æª¢æŸ¥æ§åˆ¶çµ„ 1 æ˜¯å¦å·²è¢«æ­£ç¢ºè¨­å®š
                    control_groups = obs_data.control_groups
                    if is_cc_selected_now and control_groups[1][0] == production_ai.COMMAND_CENTER_ID:
                        step_reward += 15.0 
                        has_rewarded_control_group = True
                        print("âŒ¨ï¸ æˆåŠŸå°‡ä¸»å ¡ç·¨å…¥éšŠä¼ 1ï¼çå‹µ +15")
                
                # è¢å¹•åˆ¤å®šå…µç‡ŸåŠ åˆ† (é™æ¯å±€ä¸€æ¬¡)
                if np.any((next_s_unit == production_ai.BARRACKS_ID) & (next_s_relative == 1)) and not has_rewarded_barracks:
                    step_reward += 60.0
                    has_rewarded_barracks = True
                    print("ğŸ—ï¸ è¢å¹•åµæ¸¬åˆ°å…µç‡Ÿï¼çå‹µ +60")

                # è¢å¹•åˆ¤å®šç§‘æŠ€å¯¦é©—å®¤åŠ åˆ†
                if np.any((next_s_unit == production_ai.BARRACKS_TECHLAB_ID) & (next_s_relative == 1)) and not has_rewarded_techlab:
                    step_reward += 100.0
                    has_rewarded_techlab = True
                    print("ğŸ§ª è¢å¹•åµæ¸¬åˆ°å¯¦é©—å®¤ï¼çå‹µ +100")

                # ç›®æ¨™å–®ä½ (æ å¥ªè€…) ç”¢å‡ºåŠ åˆ†
                self_m_pixels = np.sum((next_s_unit == production_ai.MARAUDER_ID) & (next_s_relative == 1))
                real_m_count = int(np.round(float(self_m_pixels) / 22.0))
                if real_m_count > last_target_count:
                    step_reward += 200.0
                    print(f"ğŸ¯ ç”¢å‡ºç‹©çµè€…ï¼æ•¸é‡: {real_m_count}")
                    last_target_count = real_m_count

                total_reward += step_reward

                # --- 5. ç‹€æ…‹æ›´æ–°èˆ‡å­˜å…¥è¨˜æ†¶ ---
                updated_block = getattr(hands, 'active_parameter', 1)
                next_state = get_state_vector(next_obs, updated_block, CURRENT_TRAIN_TASK)
                done = bool(next_obs.last() or real_m_count >= 5 or next_obs.observation.game_loop[0] >= 20160)
                
                # å°‡ç¶“é©—å­˜å…¥ deque ä¾›å¾ŒçºŒ batch è¨“ç·´
                memory.append((state, int(a_id), int(p_id), float(step_reward), next_state, bool(done)))
                
                # --- 6. æ¨¡å‹è¨“ç·´ (æ‰¹æ¬¡å­¸ç¿’) ---
                if len(memory) > 1000:
                    batch = random.sample(memory, 64)
                    # (æ­¤è™•æ‡‰åŸ·è¡Œ optimizer.step() ç­‰ DQN è¨“ç·´é‚è¼¯ï¼Œå»ºè­°ä¿ç•™ä½ åŸæœ¬çš„å¯¦ä½œ)

                if done:
                    # çµ±è¨ˆå…µç‡Ÿèˆ‡ç§‘æŠ€å¯¦é©—å®¤ (å…¨åŸŸæƒæ)
                    final_b_pixels = np.sum((next_m_unit == production_ai.BARRACKS_ID) & (next_m_relative == 1))
                    final_b_count = 1 if final_b_pixels > 0 else 0
                    
                    final_t_pixels = np.sum((next_m_unit == production_ai.BARRACKS_TECHLAB_ID) & (next_m_relative == 1))
                    final_t_count = 1 if final_t_pixels > 0 else 0
                    
                    # ã€æ ¸å¿ƒä¿®æ­£ã€‘é€™è£¡å‚³å…¥çš„åƒæ•¸é †åºå¿…é ˆèˆ‡ log_episode å®šç¾©ä¸€è‡´
                    logger.log_episode(
                        ep + 1,            # Episode (ç¬¬å¹¾æ¬¡)
                        epsilon,           # Epsilon
                        total_reward,      # ç¸½åˆ†
                        final_b_count,     # å…µç‡Ÿ
                        final_t_count,     # ç§‘æŠ€å¯¦é©—å®¤
                        real_m_count,      # æ å¥ªè€… (ç‹©çµè€…)
                        next_obs.observation.game_loop[0], # Loop
                        "Done",            # Reason
                        (production_ai.BASE_LOCATION_CODE == 1) # Location
                    )
                    
                    # æ§åˆ¶å°åŒæ­¥è¼¸å‡ºçµ±è¨ˆå…§å®¹
                    print(f"\n" + "="*40)
                    print(f"ğŸ ç¬¬ {ep+1} æ¬¡ è¨“ç·´çµç®—")
                    print(f"ğŸ  å…µç‡Ÿ: {final_b_count} | ğŸ§ª å¯¦é©—å®¤: {final_t_count} | ğŸ¯ æ å¥ªè€…: {real_m_count}")
                    print(f"ğŸ’° ç¸½åˆ†: {int(total_reward)}")
                    print("="*40 + "\n")
                    break
            
            # å›åˆçµæŸå¾Œæ›´æ–° epsilon
            epsilon = max(learn_min, epsilon * epsilon_decay)
            torch.save(brain_model.state_dict(), model_path)

if __name__ == "__main__":
    from absl import app
    app.run(main)