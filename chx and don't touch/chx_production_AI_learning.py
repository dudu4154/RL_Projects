"""
StarCraft II Q-Learning AI for Producing 5 Marauders (æ å¥ªè€…)
ä½¿ç”¨Q-learningç®—æ³•è¨“ç·´AIç”Ÿç”¢5éš»æ å¥ªè€…

é€™å€‹è…³æœ¬å¯¦ç¾äº†å®Œæ•´çš„Q-learningç³»çµ±ï¼ŒåŒ…æ‹¬ï¼š
1. ç‹€æ…‹è¡¨ç¤º - å°‡StarCraft IIéŠæˆ²ç‹€æ…‹è½‰æ›ç‚ºQ-learningå¯ç”¨çš„ç‹€æ…‹å‘é‡
2. Q-learningä»£ç† - å¯¦ç¾Q-learningç®—æ³•ï¼ŒåŒ…æ‹¬epsilon-greedyç­–ç•¥
3. çå‹µç³»çµ± - è¨­è¨ˆé¼“å‹µé«˜æ•ˆç”Ÿç”¢æ å¥ªè€…çš„çå‹µæ©Ÿåˆ¶
4. è¨“ç·´å¾ªç’° - å®Œæ•´çš„è¨“ç·´åŸºç¤æ¶æ§‹ï¼ŒåŒ…æ‹¬æ•¸æ“šè¨˜éŒ„å’Œæ€§èƒ½è¿½è¹¤
5. æ•¸æ“šå°å‡º - ä½¿ç”¨pandaså°‡è¨“ç·´æ•¸æ“šå°å‡ºç‚ºCSVæ ¼å¼ï¼Œæ–¹ä¾¿Excelåˆ†æ

ä½œè€…: Cline
æ—¥æœŸ: 2026/1/27
"""

import os
import random
import numpy as np
import pandas as pd
import csv
import time
import platform
from absl import app
from collections import deque
from datetime import datetime
import json

# Fix for random.shuffle compatibility issue
import chx_fix_random_shuffle
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from pysc2.env import sc2_env
from pysc2.lib import actions, features, units

# =========================================================
# ğŸ—ï¸ å®šç¾©äººæ—å–®ä½ ID (èˆ‡ç¾æœ‰ä»£ç¢¼ä¿æŒä¸€è‡´)
# =========================================================
COMMAND_CENTER_ID = 18  # æŒ‡æ®ä¸­å¿ƒå–®ä½ID
SUPPLY_DEPOT_ID = 19  # è£œçµ¦ç«™å–®ä½ID
REFINERY_ID = 20  # ç“¦æ–¯å» å–®ä½ID
BARRACKS_ID = 21  # å…µç‡Ÿå–®ä½ID
BARRACKS_TECHLAB_ID = 37  # å…µç‡Ÿç§‘æŠ€å¯¦é©—å®¤å–®ä½ID
SCV_ID = 45  # å·¥å…µå–®ä½ID
MARAUDER_ID = 51  # æ å¥ªè€…å–®ä½ID
MINERAL_FIELD_ID = 341  # ç¤¦ç‰©ç”°å–®ä½ID
GEYSER_ID = 342  # ç“¦æ–¯æ³‰å–®ä½ID

# =========================================================
# ğŸ“Š ç‹€æ…‹è¡¨ç¤ºé¡ - å°‡StarCraft IIéŠæˆ²ç‹€æ…‹è½‰æ›ç‚ºQ-learningç‹€æ…‹å‘é‡
# =========================================================
class StateRepresentation:
    """
    ç‹€æ…‹è¡¨ç¤ºé¡ï¼Œè² è²¬å°‡è¤‡é›œçš„StarCraft IIéŠæˆ²ç‹€æ…‹è½‰æ›ç‚ºQ-learningç®—æ³•å¯ç”¨çš„ç‹€æ…‹å‘é‡ã€‚

    ç‹€æ…‹å‘é‡åŒ…å«ä»¥ä¸‹é—œéµä¿¡æ¯ï¼š
    - è³‡æºç‹€æ…‹ï¼ˆç¤¦ç‰©ã€ç“¦æ–¯ï¼‰
    - å»ºç¯‰ç‹€æ…‹ï¼ˆè£œçµ¦ç«™ã€ç“¦æ–¯å» ã€å…µç‡Ÿã€ç§‘æŠ€å¯¦é©—å®¤ï¼‰
    - å–®ä½ç‹€æ…‹ï¼ˆSCVå·¥å…µã€æ å¥ªè€…ï¼‰
    - å·¥äººé£½å’Œç¨‹åº¦
    - ç•¶å‰å‹•ä½œå¯ç”¨æ€§
    """

    def __init__(self):
        """åˆå§‹åŒ–ç‹€æ…‹è¡¨ç¤ºé¡"""
        self.state_dim = 22  # ç‹€æ…‹å‘é‡çš„ç¶­åº¦ (12 base features + 10 one-hot action encoding)
        self.previous_state = None
        self.state_history = []

    def get_state_vector(self, obs, action_id):
        """
        å¾è§€å¯Ÿç‹€æ…‹å’Œç•¶å‰å‹•ä½œIDç”Ÿæˆç‹€æ…‹å‘é‡

        åƒæ•¸:
        - obs: ç•¶å‰éŠæˆ²è§€å¯Ÿç‹€æ…‹
        - action_id: ç•¶å‰å‹•ä½œID

        è¿”å›:
        - state_vector: æ­£è¦åŒ–å¾Œçš„ç‹€æ…‹å‘é‡
        """
        unit_type = obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]
        player = obs.observation.player
        available = obs.observation.available_actions

        # 1. è³‡æºç‹€æ…‹ï¼ˆ0-1æ­£è¦åŒ–ï¼‰
        minerals_norm = min(player.minerals / 1000.0, 1.0)  # ç¤¦ç‰©æ­£è¦åŒ–
        vespene_norm = min(player.vespene / 500.0, 1.0)    # ç“¦æ–¯æ­£è¦åŒ–

        # 2. å»ºç¯‰ç‹€æ…‹ï¼ˆäºŒé€²åˆ¶è¡¨ç¤ºï¼‰
        # æª¢æŸ¥å»ºç¯‰ç‰©æ˜¯å¦å­˜åœ¨
        barracks_built = 1.0 if np.sum(unit_type == BARRACKS_ID) > 0 else 0.0
        techlab_built = 1.0 if np.sum(unit_type == BARRACKS_TECHLAB_ID) > 0 else 0.0
        refinery_built = 1.0 if np.sum(unit_type == REFINERY_ID) > 0 else 0.0

        # 3. å–®ä½ç‹€æ…‹ï¼ˆæ­£è¦åŒ–è¨ˆæ•¸ï¼‰
        scv_count = min(np.sum(unit_type == SCV_ID) / 50.0, 1.0)  # å·¥å…µæ•¸é‡æ­£è¦åŒ–
        marauder_count = min(np.sum(unit_type == MARAUDER_ID) / 20.0, 1.0)  # æ å¥ªè€…æ•¸é‡æ­£è¦åŒ–

        # 4. å·¥äººé£½å’Œç¨‹åº¦ï¼ˆ0-1æ­£è¦åŒ–ï¼‰
        current_workers = player.food_workers
        refinery_pixels = np.sum(unit_type == REFINERY_ID)
        refinery_count = int(refinery_pixels / 80)  # 80åƒç´ ç´„ç‚ºä¸€å€‹å»ºç¯‰å¤§å°
        ideal_workers = 16 + (refinery_count * 3)
        worker_saturation = min(current_workers / max(ideal_workers, 1), 1.0)

        # 5. ä¾›æ‡‰ç‹€æ…‹ï¼ˆ0-1æ­£è¦åŒ–ï¼‰
        supply_used_norm = player.food_used / 200.0  # ä¾›æ‡‰ä½¿ç”¨æ¯”ä¾‹
        supply_cap_norm = player.food_cap / 200.0    # ä¾›æ‡‰ä¸Šé™æ¯”ä¾‹

        # 6. å‹•ä½œå¯ç”¨æ€§ï¼ˆäºŒé€²åˆ¶è¡¨ç¤ºï¼‰
        action_available = 1.0 if self._is_action_available(action_id, available) else 0.0

        # 7. ç•¶å‰å‹•ä½œIDï¼ˆone-hotç·¨ç¢¼ï¼‰
        action_onehot = np.zeros(10)
        if 0 <= action_id <= 9:
            action_onehot[action_id] = 1.0

        # 8. æ™‚é–“é€²åº¦ï¼ˆ0-1æ­£è¦åŒ–ï¼‰
        # ç¢ºä¿å¾numpyæ•¸çµ„ä¸­æå–æ¨™é‡å€¼
        game_loop_scalar = obs.observation.game_loop.item() if hasattr(obs.observation.game_loop, 'item') else obs.observation.game_loop
        time_progress = min(game_loop_scalar / (60 * 60 * 10), 1.0)  # 10åˆ†é˜éŠæˆ²æ™‚é–“æ­£è¦åŒ–

        # æ§‹å»ºå®Œæ•´ç‹€æ…‹å‘é‡ - é€æ­¥æ§‹å»ºä»¥é¿å…æ•¸çµ„å½¢ç‹€å•é¡Œ
        base_state = np.array([
            float(minerals_norm),           # 0: ç¤¦ç‰©ç‹€æ…‹
            float(vespene_norm),            # 1: ç“¦æ–¯ç‹€æ…‹
            float(barracks_built),          # 2: å…µç‡Ÿæ˜¯å¦å»ºé€ 
            float(techlab_built),           # 3: ç§‘æŠ€å¯¦é©—å®¤æ˜¯å¦å»ºé€ 
            float(refinery_built),          # 4: ç“¦æ–¯å» æ˜¯å¦å»ºé€ 
            float(scv_count),               # 5: å·¥å…µæ•¸é‡
            float(marauder_count),          # 6: æ å¥ªè€…æ•¸é‡
            float(worker_saturation),       # 7: å·¥äººé£½å’Œç¨‹åº¦
            float(supply_used_norm),        # 8: ä¾›æ‡‰ä½¿ç”¨æ¯”ä¾‹
            float(supply_cap_norm),         # 9: ä¾›æ‡‰ä¸Šé™æ¯”ä¾‹
            float(action_available),        # 10: å‹•ä½œæ˜¯å¦å¯ç”¨
            float(time_progress)            # 11: æ™‚é–“é€²åº¦
        ], dtype=np.float32)

        # æ·»åŠ å‹•ä½œone-hotç·¨ç¢¼
        state_vector = np.concatenate([base_state, action_onehot.astype(np.float32)])

        # å­˜å„²ç‹€æ…‹æ­·å²ä»¥ä¾›åˆ†æ
        self.state_history.append(state_vector.copy())

        # é™åˆ¶ç‹€æ…‹æ­·å²é•·åº¦
        if len(self.state_history) > 1000:
            self.state_history.pop(0)

        return state_vector

    def _is_action_available(self, action_id, available_actions):
        """æª¢æŸ¥æŒ‡å®šå‹•ä½œæ˜¯å¦å¯ç”¨"""
        action_mapping = {
            1: actions.FUNCTIONS.Train_SCV_quick.id,
            2: actions.FUNCTIONS.Build_SupplyDepot_screen.id,
            3: actions.FUNCTIONS.Build_Refinery_screen.id,
            4: actions.FUNCTIONS.Harvest_Gather_screen.id,
            5: actions.FUNCTIONS.Build_Barracks_screen.id,
            6: actions.FUNCTIONS.Build_TechLab_quick.id,
            7: actions.FUNCTIONS.Train_Marauder_quick.id,
            8: actions.FUNCTIONS.move_camera.id,
            9: actions.FUNCTIONS.Build_CommandCenter_screen.id
        }

        if action_id == 0:
            return True  # no_opç¸½æ˜¯å¯ç”¨

        target_action_id = action_mapping.get(action_id, None)
        return target_action_id is not None and target_action_id in available_actions

    def get_state_dimension(self):
        """è¿”å›ç‹€æ…‹å‘é‡çš„ç¶­åº¦"""
        return self.state_dim

    def clear_history(self):
        """æ¸…é™¤ç‹€æ…‹æ­·å²"""
        self.state_history = []

# =========================================================
# ğŸ§  Q-Learningä»£ç† - æ ¸å¿ƒå­¸ç¿’ç®—æ³•
# =========================================================
class QLearningAgent:
    """
    Q-Learningä»£ç†ï¼Œå¯¦ç¾å®Œæ•´çš„Q-learningç®—æ³•ï¼ŒåŒ…æ‹¬ï¼š
    - Qè¡¨ç®¡ç†
    - Epsilon-greedyç­–ç•¥
    - ç¶“é©—å›æ”¾
    - å­¸ç¿’ç‡è¡°æ¸›
    - æ¢ç´¢ç‡è¡°æ¸›
    """

    def __init__(self, state_dim, action_dim=10, learning_rate=0.1, discount_factor=0.95,
                 exploration_rate=1.0, min_exploration_rate=0.01, exploration_decay=0.995):
        """
        åˆå§‹åŒ–Q-learningä»£ç†

        åƒæ•¸:
        - state_dim: ç‹€æ…‹å‘é‡ç¶­åº¦
        - action_dim: å‹•ä½œç©ºé–“å¤§å°ï¼ˆé»˜èª10å€‹å‹•ä½œï¼‰
        - learning_rate: å­¸ç¿’ç‡ï¼ˆalphaï¼‰
        - discount_factor: æŠ˜æ‰£å› å­ï¼ˆgammaï¼‰
        - exploration_rate: åˆå§‹æ¢ç´¢ç‡ï¼ˆepsilonï¼‰
        - min_exploration_rate: æœ€å°æ¢ç´¢ç‡
        - exploration_decay: æ¢ç´¢ç‡è¡°æ¸›é€Ÿåº¦
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate
        self.min_exploration_rate = min_exploration_rate
        self.exploration_decay = exploration_decay

        # åˆå§‹åŒ–Qè¡¨
        self.q_table = {}
        self.experience_buffer = deque(maxlen=10000)  # ç¶“é©—å›æ”¾ç·©è¡å€

        # è¨“ç·´çµ±è¨ˆ
        self.episode_count = 0
        self.total_rewards = []
        self.episode_lengths = []
        self.marauders_produced_history = []

        # ç‹€æ…‹é›¢æ•£åŒ–åƒæ•¸
        self.state_bins = 20  # æ¯å€‹ç‹€æ…‹ç¶­åº¦çš„é›¢æ•£åŒ–binæ•¸é‡

    def discretize_state(self, state_vector):
        """
        å°‡é€£çºŒç‹€æ…‹å‘é‡é›¢æ•£åŒ–ç‚ºå¯ç”¨æ–¼Qè¡¨çš„é›¢æ•£ç‹€æ…‹

        åƒæ•¸:
        - state_vector: é€£çºŒç‹€æ…‹å‘é‡

        è¿”å›:
        - discretized_state: é›¢æ•£åŒ–å¾Œçš„ç‹€æ…‹å…ƒçµ„
        """
        # å°‡ç‹€æ…‹å‘é‡é›¢æ•£åŒ–ç‚ºæ•´æ•¸bin
        discretized = []
        for i, val in enumerate(state_vector):
            if i < 12:  # å‰12å€‹ç¶­åº¦æ˜¯é€£çºŒå€¼
                bin_index = int(val * self.state_bins)
                discretized.append(bin_index)
            else:  # å¾Œé¢æ˜¯one-hotç·¨ç¢¼ï¼Œç›´æ¥ä½¿ç”¨
                discretized.append(int(val))

        return tuple(discretized)

    def get_q_value(self, state, action):
        """
        è·å–æŒ‡å®šç‹€æ…‹å’Œå‹•ä½œçš„Qå€¼

        åƒæ•¸:
        - state: é›¢æ•£åŒ–ç‹€æ…‹
        - action: å‹•ä½œID

        è¿”å›:
        - Qå€¼
        """
        if state not in self.q_table:
            # åˆå§‹åŒ–æ–°ç‹€æ…‹çš„Qå€¼
            self.q_table[state] = np.zeros(self.action_dim)
        return self.q_table[state][action]

    def update_q_value(self, state, action, reward, next_state, done):
        """
        ä½¿ç”¨Q-learningæ›´æ–°è¦å‰‡æ›´æ–°Qå€¼

        åƒæ•¸:
        - state: ç•¶å‰ç‹€æ…‹
        - action: ç•¶å‰å‹•ä½œ
        - reward: ç«‹å³çå‹µ
        - next_state: ä¸‹ä¸€å€‹ç‹€æ…‹
        - done: æ˜¯å¦ç‚ºçµ‚æ­¢ç‹€æ…‹
        """
        current_q = self.get_q_value(state, action)

        if done:
            # çµ‚æ­¢ç‹€æ…‹ï¼Œæ²’æœ‰ä¸‹ä¸€å€‹ç‹€æ…‹çš„Qå€¼
            max_next_q = 0
        else:
            # ç²å–ä¸‹ä¸€å€‹ç‹€æ…‹çš„æœ€å¤§Qå€¼
            next_q_values = self.q_table.get(next_state, np.zeros(self.action_dim))
            max_next_q = np.max(next_q_values)

        # Q-learningæ›´æ–°è¦å‰‡ï¼šQ(s,a) = Q(s,a) + alpha * [r + gamma * max(Q(s',a')) - Q(s,a)]
        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)
        self.q_table[state][action] = new_q

    def select_action(self, state, available_actions=None):
        """
        ä½¿ç”¨epsilon-greedyç­–ç•¥é¸æ“‡å‹•ä½œ

        åƒæ•¸:
        - state: ç•¶å‰ç‹€æ…‹
        - available_actions: å¯ç”¨å‹•ä½œåˆ—è¡¨ï¼ˆå¯é¸ï¼‰

        è¿”å›:
        - selected_action: é¸æ“‡çš„å‹•ä½œID
        - is_exploration: æ˜¯å¦ç‚ºæ¢ç´¢å‹•ä½œ
        """
        # æ¢ç´¢ç‡è¡°æ¸›
        self.exploration_rate = max(self.min_exploration_rate,
                                  self.exploration_rate * self.exploration_decay)

        if random.random() < self.exploration_rate:
            # æ¢ç´¢ï¼šéš¨æ©Ÿé¸æ“‡å‹•ä½œ
            if available_actions is not None and len(available_actions) > 0:
                selected_action = random.choice(available_actions)
            else:
                selected_action = random.randint(0, self.action_dim - 1)
            return selected_action, True
        else:
            # åˆ©ç”¨ï¼šé¸æ“‡Qå€¼æœ€é«˜çš„å‹•ä½œ
            if state not in self.q_table:
                # å¦‚æœç‹€æ…‹ä¸åœ¨Qè¡¨ä¸­ï¼Œéš¨æ©Ÿé¸æ“‡å‹•ä½œ
                if available_actions is not None and len(available_actions) > 0:
                    selected_action = random.choice(available_actions)
                else:
                    selected_action = random.randint(0, self.action_dim - 1)
                return selected_action, False

            q_values = self.q_table[state]

            # å¦‚æœæœ‰å¯ç”¨å‹•ä½œåˆ—è¡¨ï¼Œåªè€ƒæ…®å¯ç”¨å‹•ä½œ
            if available_actions is not None and len(available_actions) > 0:
                # éæ¿¾ä¸å¯ç”¨å‹•ä½œ
                available_q_values = [q_values[a] if a in available_actions else -np.inf for a in range(self.action_dim)]
                best_action = np.argmax(available_q_values)
            else:
                best_action = np.argmax(q_values)

            return best_action, False

    def add_experience(self, state, action, reward, next_state, done):
        """
        å°‡ç¶“é©—æ·»åŠ åˆ°ç¶“é©—å›æ”¾ç·©è¡å€

        åƒæ•¸:
        - state: ç•¶å‰ç‹€æ…‹
        - action: ç•¶å‰å‹•ä½œ
        - reward: ç«‹å³çå‹µ
        - next_state: ä¸‹ä¸€å€‹ç‹€æ…‹
        - done: æ˜¯å¦ç‚ºçµ‚æ­¢ç‹€æ…‹
        """
        self.experience_buffer.append((state, action, reward, next_state, done))

    def train_from_experience(self, batch_size=32):
        """
        å¾ç¶“é©—å›æ”¾ç·©è¡å€ä¸­è¨“ç·´

        åƒæ•¸:
        - batch_size: æ¯æ‰¹è¨“ç·´çš„ç¶“é©—æ•¸é‡
        """
        if len(self.experience_buffer) < batch_size:
            return  # ç¶“é©—ä¸è¶³ï¼Œä¸è¨“ç·´

        # éš¨æ©Ÿæ¡æ¨£ç¶“é©—
        batch = random.sample(self.experience_buffer, batch_size)

        for state, action, reward, next_state, done in batch:
            self.update_q_value(state, action, reward, next_state, done)

    def save_model(self, filename):
        """
        ä¿å­˜Qè¡¨æ¨¡å‹åˆ°æ–‡ä»¶

        åƒæ•¸:
        - filename: ä¿å­˜æ–‡ä»¶å
        """
        with open(filename, 'w') as f:
            json.dump({str(k): v.tolist() for k, v in self.q_table.items()}, f)

    def load_model(self, filename):
        """
        å¾æ–‡ä»¶åŠ è¼‰Qè¡¨æ¨¡å‹

        åƒæ•¸:
        - filename: åŠ è¼‰æ–‡ä»¶å
        """
        if os.path.exists(filename):
            with open(filename, 'r') as f:
                data = json.load(f)
                self.q_table = {eval(k): np.array(v) for k, v in data.items()}

    def get_exploration_rate(self):
        """ç²å–ç•¶å‰æ¢ç´¢ç‡"""
        return self.exploration_rate

    def increment_episode(self):
        """å¢åŠ å›åˆè¨ˆæ•¸"""
        self.episode_count += 1

    def add_reward(self, reward):
        """æ·»åŠ çå‹µåˆ°æ­·å²è¨˜éŒ„"""
        self.total_rewards.append(reward)

    def add_episode_length(self, length):
        """æ·»åŠ å›åˆé•·åº¦åˆ°æ­·å²è¨˜éŒ„"""
        self.episode_lengths.append(length)

    def add_marauders_produced(self, count):
        """æ·»åŠ æ å¥ªè€…ç”Ÿç”¢æ•¸é‡åˆ°æ­·å²è¨˜éŒ„"""
        self.marauders_produced_history.append(count)

# =========================================================
# ğŸ çå‹µç³»çµ± - è¨­è¨ˆé¼“å‹µé«˜æ•ˆç”Ÿç”¢æ å¥ªè€…çš„çå‹µæ©Ÿåˆ¶
# =========================================================
class RewardSystem:
    """
    çå‹µç³»çµ±ï¼Œè¨­è¨ˆç”¨æ–¼é¼“å‹µAIé«˜æ•ˆç”Ÿç”¢5éš»æ å¥ªè€…çš„çå‹µæ©Ÿåˆ¶ã€‚

    çå‹µè¨­è¨ˆåŸå‰‡ï¼š
    - æ­£å‘çå‹µï¼šå®Œæˆé—œéµæ­¥é©Ÿå’Œç›®æ¨™
    - è² å‘çå‹µï¼šè³‡æºæµªè²»å’Œä½æ•ˆè¡Œç‚º
    - æ™‚é–“æ‡²ç½°ï¼šé¼“å‹µå¿«é€Ÿå®Œæˆç›®æ¨™
    - çµ‚æ­¢çå‹µï¼šå®Œæˆ5éš»æ å¥ªè€…ç”Ÿç”¢
    """

    def __init__(self):
        """åˆå§‹åŒ–çå‹µç³»çµ±"""
        self.previous_marauders = 0
        self.previous_minerals = 0
        self.previous_vespene = 0
        self.start_time = None
        self.episode_start_time = None
        # è¿½è¹¤å»ºç¯‰ç‰©æ­·å²æœ€å¤§æ•¸é‡
        self.max_supply_depots = 0
        self.max_barracks = 0
        self.max_techlabs = 0
        self.max_refineries = 0
        # è¿½è¹¤è£œçµ¦ç«™æ•¸é‡ï¼ˆç”¨æ–¼ä¸Šé™æ©Ÿåˆ¶ï¼‰
        self.supply_depot_count = 0
        # è¿½è¹¤å»ºç¯‰ç‰©å®Œæˆç‹€æ…‹
        self.barracks_completed = False
        self.techlab_completed = False
        self.refinery_completed = False

    def calculate_reward(self, obs, action_id, marauders_produced, done=False):
        """
        è¨ˆç®—ç•¶å‰æ­¥é©Ÿçš„çå‹µ

        åƒæ•¸:
        - obs: ç•¶å‰éŠæˆ²è§€å¯Ÿç‹€æ…‹
        - action_id: ç•¶å‰å‹•ä½œID
        - marauders_produced: ç•¶å‰æ å¥ªè€…ç”Ÿç”¢æ•¸é‡
        - done: æ˜¯å¦ç‚ºçµ‚æ­¢ç‹€æ…‹

        è¿”å›:
        - reward: è¨ˆç®—å¾—åˆ°çš„çå‹µå€¼
        """
        player = obs.observation.player
        unit_type = obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]

        # åˆå§‹åŒ–çå‹µ
        reward = 0

        # 1. æ™‚é–“æ‡²ç½°ï¼ˆæ¯æ­¥å°æ‡²ç½°ï¼Œé¼“å‹µå¿«é€Ÿå®Œæˆï¼‰
        reward -= 0.1

        # 2. è¨ˆç®—ç•¶å‰å»ºç¯‰ç‰©æ•¸é‡
        current_supply_depots = np.sum(unit_type == SUPPLY_DEPOT_ID)
        current_barracks = np.sum(unit_type == BARRACKS_ID)
        current_techlabs = np.sum(unit_type == BARRACKS_TECHLAB_ID)
        current_refineries = np.sum(unit_type == REFINERY_ID)

        # 3. æ›´æ–°æ­·å²æœ€å¤§å€¼
        if current_supply_depots > self.max_supply_depots:
            self.max_supply_depots = current_supply_depots
        if current_barracks > self.max_barracks:
            self.max_barracks = current_barracks
        if current_techlabs > self.max_techlabs:
            self.max_techlabs = current_techlabs
        if current_refineries > self.max_refineries:
            self.max_refineries = current_refineries

        # 4. æ–°çš„çå‹µç³»çµ± - åªæœ‰ç•¶ç¾åœ¨æ•¸é‡ > æ­·å²æœ€é«˜æ•¸é‡æ™‚æ‰çµ¦åˆ†
        # é€ å‡ºä¸€éš»æ å¥ªè€… +50 (å¤§ç) - çµ‚æ¥µç›®æ¨™ï¼Œåˆ†æ•¸æœ€é«˜
        if marauders_produced > self.previous_marauders:
            reward += 50 * (marauders_produced - self.previous_marauders)

        # è“‹å‡ºå…µç‡Ÿ +10 (ä¸­ç) - é—œéµè·¯å¾‘
        if current_barracks > self.max_barracks:  # åªæœ‰æ–°å»ºæˆçš„å…µç‡Ÿæ‰çµ¦åˆ†
            reward += 10

        # è“‹å‡ºç§‘æŠ€å¯¦é©—å®¤ +10 (ä¸­ç) - è§£é–æ å¥ªè€…çš„é‘°åŒ™
        if current_techlabs > self.max_techlabs:  # åªæœ‰æ–°å»ºæˆçš„ç§‘æŠ€å¯¦é©—å®¤æ‰çµ¦åˆ†
            reward += 10

        # è“‹å‡ºç“¦æ–¯å»  +5 (å°ç) - æœ‰ç“¦æ–¯æ‰èƒ½é€ å…µ
        if current_refineries > self.max_refineries:  # åªæœ‰æ–°å»ºæˆçš„ç“¦æ–¯å» æ‰çµ¦åˆ†
            reward += 5

        # è“‹å‡ºè£œçµ¦ç«™ +2 (å°ç) - æœ‰äººå£æ‰èƒ½é€ å…µï¼Œä½†ä¸è¦çµ¦å¤ªé«˜
        # åªæœ‰å‰3å€‹è£œçµ¦ç«™çµ¦åˆ†ï¼Œç¬¬4å€‹é–‹å§‹ +0 åˆ†ï¼ˆä¸Šé™æ©Ÿåˆ¶ï¼‰
        if current_supply_depots > self.max_supply_depots:  # åªæœ‰æ–°å»ºæˆçš„è£œçµ¦ç«™æ‰çµ¦åˆ†
            if self.max_supply_depots < 3:  # ä¸Šé™æ©Ÿåˆ¶ï¼šåªæœ‰å‰3å€‹è£œçµ¦ç«™çµ¦åˆ†
                reward += 2
            else:
                # For depots beyond 3, give negative reward to discourage overbuilding
                reward -= 1

        # é€ å‡ºä¸€éš»å·¥å…µ (SCV) +1 (å°å°ç) - ç¶“æ¿ŸåŸºç¤
        current_scvs = np.sum(unit_type == SCV_ID)
        if current_scvs > np.sum(self.previous_scvs if hasattr(self, 'previous_scvs') else 0):
            reward += 1
        self.previous_scvs = current_scvs

        # 5. ç„¡æ•ˆå‹•ä½œ (éŒ¢ä¸å¤ äº‚æŒ‰) -1 (æ‡²ç½°)
        # æª¢æŸ¥å‹•ä½œæ˜¯å¦å› ç‚ºè³‡æºä¸è¶³è€Œå¤±æ•—
        if action_id != 0:  # æ’é™¤no_op
            # æª¢æŸ¥å¸¸è¦‹çš„è³‡æºä¸è¶³æƒ…æ³
            if action_id == 1 and player.minerals < 50:  # è¨“ç·´SCVéœ€è¦50ç¤¦ç‰©
                reward -= 1
            elif action_id == 2 and player.minerals < 100:  # å»ºé€ è£œçµ¦ç«™éœ€è¦100ç¤¦ç‰©
                reward -= 1
            elif action_id == 3 and player.minerals < 75:  # å»ºé€ ç“¦æ–¯å» éœ€è¦75ç¤¦ç‰©
                reward -= 1
            elif action_id == 5 and player.minerals < 150:  # å»ºé€ å…µç‡Ÿéœ€è¦150ç¤¦ç‰©
                reward -= 1
            elif action_id == 6 and (player.minerals < 50 or player.vespene < 25):  # ç§‘æŠ€å¯¦é©—å®¤éœ€è¦50ç¤¦ç‰©+25ç“¦æ–¯
                reward -= 1
            elif action_id == 7 and (player.minerals < 100 or player.vespene < 25):  # æ å¥ªè€…éœ€è¦100ç¤¦ç‰©+25ç“¦æ–¯
                reward -= 1

        # 6. å®Œæˆ5éš»æ å¥ªè€…çš„çµ‚æ¥µçå‹µï¼ˆä¿ç•™åŸæœ‰é‚è¼¯ï¼‰
        if done and marauders_produced >= 5:
            reward += 50.0  # å®Œæˆç›®æ¨™çš„å¤§çå‹µ

        # 7. å®Œæˆç›®æ¨™ä½†ç”¨æ™‚éé•·çš„æ‡²ç½°ï¼ˆä¿ç•™åŸæœ‰é‚è¼¯ï¼‰
        if done and marauders_produced >= 5:
            current_time = time.time()
            if self.episode_start_time is not None:
                episode_duration = current_time - self.episode_start_time
                # æ¯ç§’é¡å¤–æ™‚é–“æ‡²ç½°
                time_penalty = max(0, episode_duration - 300) * 0.01  # 5åˆ†é˜ä»¥ä¸Šé–‹å§‹æ‡²ç½°
                reward -= time_penalty

        # æ›´æ–°ä¹‹å‰çš„ç‹€æ…‹
        self.previous_marauders = marauders_produced
        self.previous_minerals = player.minerals
        self.previous_vespene = player.vespene

        return reward

    def reset(self):
        """é‡ç½®çå‹µç³»çµ±ç‹€æ…‹

        åœ¨æ¯å€‹å›åˆé–‹å§‹æ™‚èª¿ç”¨ï¼Œé‡ç½®æ‰€æœ‰ç‹€æ…‹è·Ÿè¹¤è®Šé‡
        ç¢ºä¿æ¯å€‹å›åˆçš„çå‹µè¨ˆç®—æ˜¯ç¨ç«‹çš„ï¼Œä¸å—ä¸Šä¸€å›åˆå½±éŸ¿
        """
        self.previous_marauders = 0  # é‡ç½®æ å¥ªè€…è¨ˆæ•¸
        self.previous_minerals = 0   # é‡ç½®ç¤¦ç‰©è·Ÿè¹¤
        self.previous_vespene = 0    # é‡ç½®ç“¦æ–¯è·Ÿè¹¤
        self.previous_scvs = 0       # é‡ç½®å·¥å…µè·Ÿè¹¤
        self.episode_start_time = time.time()  # è¨˜éŒ„æ–°å›åˆçš„é–‹å§‹æ™‚é–“

        # é‡ç½®å»ºç¯‰ç‰©æ­·å²æœ€å¤§æ•¸é‡
        self.max_supply_depots = 0
        self.max_barracks = 0
        self.max_techlabs = 0
        self.max_refineries = 0

        # é‡ç½®å»ºç¯‰ç‰©å®Œæˆç‹€æ…‹
        self.barracks_completed = False
        self.techlab_completed = False
        self.refinery_completed = False

# =========================================================
# ğŸ“Š æ•¸æ“šè¨˜éŒ„å™¨ - ä½¿ç”¨pandasè¨˜éŒ„è¨“ç·´æ•¸æ“šä¸¦å°å‡ºCSV
# =========================================================
class DataLogger:
    """
    æ•¸æ“šè¨˜éŒ„å™¨ï¼Œä½¿ç”¨pandasè¨˜éŒ„å®Œæ•´çš„è¨“ç·´æ•¸æ“šï¼Œä¸¦å°å‡ºç‚ºCSVæ ¼å¼ä¾›Excelåˆ†æã€‚

    è¨˜éŒ„çš„æ•¸æ“šåŒ…æ‹¬ï¼š
    - è¨“ç·´æŒ‡æ¨™ï¼ˆçå‹µã€Qå€¼ã€å›åˆé•·åº¦ï¼‰
    - æ€§èƒ½çµ±è¨ˆï¼ˆæ å¥ªè€…ç”Ÿç”¢æ™‚é–“ã€æˆåŠŸç‡ï¼‰
    - å­¸ç¿’æ›²ç·š
    - å‹•ä½œåˆ†ä½ˆ
    """

    def __init__(self):
        """åˆå§‹åŒ–æ•¸æ“šè¨˜éŒ„å™¨"""
        # å‰µå»ºlogsç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not os.path.exists("logs"):
            os.makedirs("logs")

        # å‰µå»ºAI_learningç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not os.path.exists("logs/AI_learning"):
            os.makedirs("logs/AI_learning")

        # è¨­ç½®æ–‡ä»¶åï¼ŒåŒ…å«æ™‚é–“æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.csv_filename = f"logs/AI_learning/ql_training_{timestamp}.csv"
        self.json_filename = f"logs/AI_learning/ql_stats_{timestamp}.json"

        # åˆå§‹åŒ–æ•¸æ“šæ¡†æ¶ - åªè¨˜éŒ„å›åˆç´šåˆ¥çµ±è¨ˆæ•¸æ“šï¼Œé©åˆExcelåˆ†æ
        self.episode_data = pd.DataFrame(columns=[
            'Episode', 'Total_Reward', 'Episode_Length',
            'Marauders_Produced', 'Success', 'Exploration_Rate',
            'Timestamp', 'Training_Time'
        ])

        # çµ±è¨ˆæ•¸æ“š
        self.stats = {
            'episodes': [],
            'total_rewards': [],
            'avg_rewards': [],
            'max_rewards': [],
            'min_rewards': [],
            'episode_lengths': [],
            'marauders_produced': [],
            'success_rate': [],
            'avg_exploration_rate': [],
            'training_time': [],
            'timestamp': []
        }

        # è¨ˆæ™‚å™¨
        self.start_time = time.time()
        self.episode_start_time = time.time()

    def log_step(self, episode, step, obs, action_id, reward, q_value, exploration_rate):
        """
        è¨˜éŒ„å–®æ­¥è¨“ç·´æ•¸æ“š - ç¾åœ¨åªè¨˜éŒ„å›åˆç´šåˆ¥çµ±è¨ˆï¼Œä¸è¨˜éŒ„æ¯æ­¥è©³ç´°æ•¸æ“š

        åƒæ•¸:
        - episode: ç•¶å‰å›åˆæ•¸
        - step: ç•¶å‰æ­¥æ•¸
        - obs: ç•¶å‰éŠæˆ²è§€å¯Ÿç‹€æ…‹
        - action_id: ç•¶å‰å‹•ä½œID
        - reward: ç•¶å‰çå‹µ
        - q_value: ç•¶å‰Qå€¼
        - exploration_rate: ç•¶å‰æ¢ç´¢ç‡
        """
        # ä¸å†è¨˜éŒ„æ¯æ­¥æ•¸æ“šï¼Œåªåœ¨å›åˆçµæŸæ™‚è¨˜éŒ„çµ±è¨ˆæ•¸æ“š
        pass

    def log_episode_stats(self, episode, total_reward, episode_length, marauders_produced, success):
        """
        è¨˜éŒ„å›åˆçµ±è¨ˆæ•¸æ“š

        åƒæ•¸:
        - episode: å›åˆæ•¸
        - total_reward: ç¸½çå‹µ
        - episode_length: å›åˆé•·åº¦
        - marauders_produced: ç”Ÿç”¢çš„æ å¥ªè€…æ•¸é‡
        - success: æ˜¯å¦æˆåŠŸå®Œæˆç›®æ¨™
        """
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        training_duration = time.time() - self.start_time
        episode_duration = time.time() - self.episode_start_time

        # è¨˜éŒ„å›åˆç´šåˆ¥æ•¸æ“šåˆ°episode_data DataFrame
        episode_data_row = {
            'Episode': episode + 1,  # ä½¿ç”¨1-based indexing
            'Total_Reward': total_reward,
            'Episode_Length': episode_length,
            'Marauders_Produced': marauders_produced,
            'Success': 1 if success else 0,
            'Exploration_Rate': self.stats.get('exploration_rates', [0.1])[-1] if 'exploration_rates' in self.stats and self.stats['exploration_rates'] else 0.1,
            'Timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'Training_Time': training_duration
        }

        # æ·»åŠ åˆ°episode_data DataFrame
        self.episode_data = pd.concat([
            self.episode_data,
            pd.DataFrame([episode_data_row])
        ], ignore_index=True)

        # æ›´æ–°çµ±è¨ˆæ•¸æ“š
        self.stats['episodes'].append(episode)
        self.stats['total_rewards'].append(total_reward)
        self.stats['episode_lengths'].append(episode_length)
        self.stats['marauders_produced'].append(marauders_produced)
        self.stats['success_rate'].append(1.0 if success else 0.0)
        self.stats['training_time'].append(training_duration)
        self.stats['timestamp'].append(datetime.now().strftime("%Y-%m-%d %H:%M:%S"))

        # é‡ç½®å›åˆè¨ˆæ™‚å™¨
        self.episode_start_time = time.time()

    def save_to_csv(self):
        """
        å°‡è¨“ç·´æ•¸æ“šä¿å­˜ç‚ºCSVæ–‡ä»¶ - ç¾åœ¨ä¿å­˜å›åˆç´šåˆ¥çµ±è¨ˆæ•¸æ“š
        """
        if not self.episode_data.empty:
            # ä¿å­˜å›åˆç´šåˆ¥çµ±è¨ˆæ•¸æ“šåˆ°CSVæ–‡ä»¶ï¼Œé©åˆExcelæ‰“é–‹
            self.episode_data.to_csv(self.csv_filename, index=False, encoding='utf-8-sig')
            print(f"âœ… å›åˆçµ±è¨ˆæ•¸æ“šå·²ä¿å­˜åˆ°: {self.csv_filename}ï¼ˆå…±{len(self.episode_data)}ç­†æ•¸æ“šï¼Œå¯ç”¨Excelæ‰“é–‹ï¼‰")
        else:
            print(f"âš ï¸ æ²’æœ‰å›åˆæ•¸æ“šå¯ä¿å­˜")

    def save_stats_to_json(self):
        """
        å°‡çµ±è¨ˆæ•¸æ“šä¿å­˜ç‚ºJSONæ–‡ä»¶
        """
        # è¨ˆç®—é¡å¤–çµ±è¨ˆæ•¸æ“š
        if self.stats['total_rewards']:
            self.stats['avg_rewards'] = np.mean(self.stats['total_rewards'])
            self.stats['max_rewards'] = np.max(self.stats['total_rewards'])
            self.stats['min_rewards'] = np.min(self.stats['total_rewards'])
            self.stats['avg_exploration_rate'] = np.mean(self.stats.get('exploration_rates', [0.1]))

        with open(self.json_filename, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
            print(f"âœ… çµ±è¨ˆæ•¸æ“šå·²ä¿å­˜åˆ°: {self.json_filename}")

    def get_training_summary(self):
        """
        è·å–è¨“ç·´æ‘˜è¦ä¿¡æ¯

        è¿”å›:
        - summary: è¨“ç·´æ‘˜è¦å­—å…¸
        """
        if len(self.stats['episodes']) == 0:
            return {}

        summary = {
            'total_episodes': len(self.stats['episodes']),
            'total_steps': len(self.episode_data),  # ç¾åœ¨ä½¿ç”¨å›åˆæ•¸æ“šé•·åº¦
            'avg_reward': np.mean(self.stats['total_rewards']),
            'max_reward': np.max(self.stats['total_rewards']),
            'min_reward': np.min(self.stats['total_rewards']),
            'avg_episode_length': np.mean(self.stats['episode_lengths']),
            'total_marauders': np.sum(self.stats['marauders_produced']),
            'success_rate': np.mean(self.stats['success_rate']),
            'total_training_time': time.time() - self.start_time,
            'csv_file': self.csv_filename,
            'json_file': self.json_filename
        }

        return summary

    def add_exploration_rate(self, exploration_rate):
        """
        æ·»åŠ æ¢ç´¢ç‡åˆ°çµ±è¨ˆæ•¸æ“š

        åƒæ•¸:
        - exploration_rate: ç•¶å‰æ¢ç´¢ç‡
        """
        if 'exploration_rates' not in self.stats:
            self.stats['exploration_rates'] = []
        self.stats['exploration_rates'].append(exploration_rate)

# =========================================================
# ğŸ¤– Q-Learningç”Ÿç”¢AI - æ•´åˆQ-learningèˆ‡ç¾æœ‰ProductionAI
# =========================================================
class QLearningProductionAI:
    """
    æ•´åˆQ-learningèˆ‡ç¾æœ‰ProductionAIçš„å®Œæ•´AIç³»çµ±ã€‚

    é€™å€‹é¡æ•´åˆäº†ï¼š
    - ç‹€æ…‹è¡¨ç¤º
    - Q-learningä»£ç†
    - çå‹µç³»çµ±
    - æ•¸æ“šè¨˜éŒ„
    - èˆ‡StarCraft IIç’°å¢ƒçš„äº¤äº’
    """

    def __init__(self):
        """åˆå§‹åŒ–Q-learningç”Ÿç”¢AI"""
        # åˆå§‹åŒ–çµ„ä»¶
        self.state_representation = StateRepresentation()
        self.q_agent = QLearningAgent(self.state_representation.get_state_dimension())
        self.reward_system = RewardSystem()
        self.data_logger = DataLogger()

        # å¾ç¾æœ‰ProductionAIç¹¼æ‰¿çš„ç‹€æ…‹
        self.depots_built = 0
        self.refinery_targets = []
        self.cc_x_screen = 42
        self.cc_y_screen = 42
        self.gas_workers_assigned = 0
        self.base_minimap_coords = None
        self.scan_points = []
        self.current_scan_idx = 0
        self.marauders_produced = 0
        self.marauder_production_complete = False
        self.barracks_built = False
        self.techlab_built = False
        self.attempted_geyser_positions = set()
        self.current_refinery_target = None
        self.gas_worker_timer = 0

        # è¨“ç·´åƒæ•¸
        self.current_episode = 0
        self.current_step = 0
        self.total_reward = 0

    def get_action(self, obs, action_id=None):
        """
        ç²å–åŸºæ–¼Q-learningçš„å‹•ä½œ

        åƒæ•¸:
        - obs: ç•¶å‰éŠæˆ²è§€å¯Ÿç‹€æ…‹
        - action_id: å¯é¸çš„å‹•ä½œIDï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰

        è¿”å›:
        - sc2_action: StarCraft IIå‹•ä½œ
        """
        # ç²å–ç•¶å‰ç‹€æ…‹
        state_vector = self.state_representation.get_state_vector(obs, action_id or 0)
        discretized_state = self.q_agent.discretize_state(state_vector)

        # ç²å–å¯ç”¨å‹•ä½œ
        available_actions = self._get_available_actions(obs)

        # é¸æ“‡å‹•ä½œï¼ˆä½¿ç”¨Q-learningæˆ–æŒ‡å®šå‹•ä½œï¼‰
        if action_id is None:
            selected_action, is_exploration = self.q_agent.select_action(discretized_state, available_actions)
        else:
            selected_action = action_id
            is_exploration = False

        # ç²å–Qå€¼
        q_value = self.q_agent.get_q_value(discretized_state, selected_action)

        # ç²å–çå‹µ
        reward = self.reward_system.calculate_reward(
            obs, selected_action, self.marauders_produced
        )

        # è¨˜éŒ„æ•¸æ“š
        self.data_logger.log_step(
            self.current_episode, self.current_step, obs,
            selected_action, reward, q_value, self.q_agent.get_exploration_rate()
        )

        # æ›´æ–°ç¸½çå‹µ
        self.total_reward += reward

        # ç²å–StarCraft IIå‹•ä½œ
        sc2_action = self._get_sc2_action(obs, selected_action)

        # æ›´æ–°ç‹€æ…‹
        self.current_step += 1

        return sc2_action, selected_action, reward, q_value

    def _get_available_actions(self, obs):
        """
        è·å–ç•¶å‰å¯ç”¨çš„å‹•ä½œåˆ—è¡¨

        åƒæ•¸:
        - obs: ç•¶å‰éŠæˆ²è§€å¯Ÿç‹€æ…‹

        è¿”å›:
        - available_actions: å¯ç”¨å‹•ä½œIDåˆ—è¡¨
        """
        available = obs.observation.available_actions
        available_actions = []

        # æª¢æŸ¥æ¯å€‹å‹•ä½œæ˜¯å¦å¯ç”¨
        if actions.FUNCTIONS.Train_SCV_quick.id in available:
            available_actions.append(1)
        if actions.FUNCTIONS.Build_SupplyDepot_screen.id in available:
            available_actions.append(2)
        if actions.FUNCTIONS.Build_Refinery_screen.id in available:
            available_actions.append(3)
        if actions.FUNCTIONS.Harvest_Gather_screen.id in available:
            available_actions.append(4)
        if actions.FUNCTIONS.Build_Barracks_screen.id in available:
            available_actions.append(5)
        if actions.FUNCTIONS.Build_TechLab_quick.id in available:
            available_actions.append(6)
        if actions.FUNCTIONS.Train_Marauder_quick.id in available:
            available_actions.append(7)
        if actions.FUNCTIONS.move_camera.id in available:
            available_actions.append(8)
        if actions.FUNCTIONS.Build_CommandCenter_screen.id in available:
            available_actions.append(9)

        # ç¸½æ˜¯å¯ç”¨çš„å‹•ä½œ
        available_actions.append(0)  # no_op

        return available_actions

    def _get_sc2_action(self, obs, action_id):
        """
        å°‡å‹•ä½œIDè½‰æ›ç‚ºStarCraft IIå‹•ä½œ

        åƒæ•¸:
        - obs: ç•¶å‰éŠæˆ²è§€å¯Ÿç‹€æ…‹
        - action_id: å‹•ä½œID

        è¿”å›:
        - sc2_action: StarCraft IIå‹•ä½œ
        """
        unit_type = obs.observation.feature_screen[features.SCREEN_FEATURES.unit_type.index]
        player = obs.observation.player
        available = obs.observation.available_actions

        # --- 1. åº§æ¨™èˆ‡é˜²ç¦¦å‹æƒæé»åˆå§‹åŒ– ---
        if self.base_minimap_coords is None:
            player_relative_mini = obs.observation.feature_minimap[features.MINIMAP_FEATURES.player_relative.index]
            y_mini, x_mini = (player_relative_mini == features.PlayerRelative.SELF).nonzero()
            if x_mini.any():
                bx, by = int(x_mini.mean()), int(y_mini.mean())
                self.base_minimap_coords = (bx, by)
                # ä»¥åŸºåœ°ç‚ºä¸­å¿ƒæ“´æ•£çš„æƒæé»
                offsets = [(0, 0), (20, 0), (-20, 0), (0, 20), (0, -20), (15, 15), (-15, -15)]
                self.scan_points = [(np.clip(bx + dx, 0, 63), np.clip(by + dy, 0, 63)) for dx, dy in offsets]

        # --- 2. è¦–è§’è·³è½‰é‚è¼¯ ---
        cc_y, cc_x = (unit_type == COMMAND_CENTER_ID).nonzero()

        # Action 9 (é–‹ç¤¦): è‹¥ç•«é¢çœ‹å¾—åˆ°ä¸»åŸºï¼Œèªªæ˜é‚„æ²’è·³è½‰åˆ°ç¤¦å€ä½ç½®ï¼Œéœ€è¦ç§»å‹•é¡é ­
        if action_id == 9 and cc_x.any():
            if len(self.scan_points) > 1:
                return actions.FUNCTIONS.move_camera(self.scan_points[1])

        # Action 0-7 (åŸºç¤ç‡Ÿé‹): è‹¥ç•«é¢æ²’åŸºåœ°ï¼Œå¼·åˆ¶æ‹‰å›ä¸»åŸºåœ°
        if action_id <= 7 and not cc_x.any() and self.base_minimap_coords:
            return actions.FUNCTIONS.move_camera(self.base_minimap_coords)

        # æ›´æ–°åŸºåœ°åœ¨è¢å¹•ä¸­çš„åº§æ¨™ (ç”¨æ–¼è¨ˆç®—ç›¸å°å»ºç¯‰ä½ç½®)
        if cc_x.any():
            self.cc_x_screen, self.cc_y_screen = int(cc_x.mean()), int(cc_y.mean())

        # å‹•æ…‹å·¥å…µé£½å’Œè¨ˆç®—
        current_workers = player.food_workers
        refinery_pixels = np.sum(unit_type == REFINERY_ID)
        refinery_count = int(refinery_pixels / 80) # 80 åƒç´ ç´„ç‚ºä¸€å€‹å»ºç¯‰å¤§å°
        ideal_workers = 16 + (refinery_count * 3)

        # è¨ˆç®—ç•¶å‰å¯¦éš›åœ¨æ¡é›†ç“¦æ–¯çš„å·¥å…µæ•¸é‡
        gas_workers_actual = 0
        if self.refinery_targets:
            scv_y, scv_x = (unit_type == SCV_ID).nonzero()
            if scv_x.any() and scv_y.any():
                for refinery_target in self.refinery_targets:
                    if refinery_target:
                        distances = np.sqrt((scv_x - refinery_target[0])**2 + (scv_y - refinery_target[1])**2)
                        gas_workers_actual += np.sum(distances < 10)
        self.gas_workers_assigned = int(gas_workers_actual)

        # ç“¦æ–¯å·¥äººåˆ†é… - æ›´é »ç¹åœ°æª¢æŸ¥å’Œåˆ†é…å·¥äºº
        self.gas_worker_timer = (self.gas_worker_timer + 1) % 10
        if self.gas_worker_timer == 0:
            self._assign_gas_workers_if_needed(obs, unit_type)

        # æ›´æ–°å»ºç¯‰ç‰©ç‹€æ…‹
        self._update_building_status(unit_type)

        # --- 3. å°ˆæ³¨æ–¼ç”Ÿç”¢äº”éš»æ å¥ªè€…çš„é‚è¼¯ ---
        # [Action 1] è¨“ç·´ SCV (ç¶­æŒåŸºæœ¬ç¶“æ¿Ÿ)
        if action_id == 1:
            if current_workers < ideal_workers and player.minerals >= 50:
                if actions.FUNCTIONS.Train_SCV_quick.id in available:
                    return actions.FUNCTIONS.Train_SCV_quick("now")
            return self._select_unit(unit_type, COMMAND_CENTER_ID)

        # [Action 2] å»ºé€ è£œçµ¦ç«™ (ç¢ºä¿æœ‰è¶³å¤ è£œçµ¦)
        elif action_id == 2:
            if player.minerals >= 100 and actions.FUNCTIONS.Build_SupplyDepot_screen.id in available:
                target = self._calc_depot_pos(unit_type)
                return actions.FUNCTIONS.Build_SupplyDepot_screen("now", target)
            return self._select_scv(unit_type)

        # [Action 3] å»ºé€ ç“¦æ–¯å»  (æ å¥ªè€…éœ€è¦ç“¦æ–¯)
        elif action_id == 3:
            all_geysers = self._find_all_geysers(unit_type)

            # å¦‚æœæ²’æœ‰æ‰¾åˆ°ä»»ä½•ç“¦æ–¯æ³‰ï¼Œå˜—è©¦ç§»å‹•ç›¸æ©Ÿä¾†å°‹æ‰¾
            if not all_geysers and self.base_minimap_coords:
                next_camera_pos = self._get_next_camera_position_for_geysers()
                return actions.FUNCTIONS.move_camera(next_camera_pos)

            # å¦‚æœæ‰¾åˆ°ç“¦æ–¯æ³‰ï¼Œæª¢æŸ¥å“ªäº›ç“¦æ–¯æ³‰é‚„æ²’æœ‰å»ºé€ ç“¦æ–¯å» 
            if all_geysers and player.minerals >= 75 and actions.FUNCTIONS.Build_Refinery_screen.id in available:
                geysers_without_refineries = []
                for geyser_pos in all_geysers:
                    has_refinery = False
                    for refinery_target in self.refinery_targets:
                        if refinery_target and np.sqrt((geyser_pos[0] - refinery_target[0])**2 + (geyser_pos[1] - refinery_target[1])**2) < 15:
                            has_refinery = True
                            break
                    if not has_refinery:
                        geysers_without_refineries.append(geyser_pos)

                # å¦‚æœæœ‰ç“¦æ–¯æ³‰æ²’æœ‰ç“¦æ–¯å» ï¼Œå»ºé€ åœ¨ç¬¬ä¸€å€‹é€™æ¨£çš„ç“¦æ–¯æ³‰ä¸Š
                if geysers_without_refineries:
                    target_geyser = geysers_without_refineries[0]
                    if target_geyser not in self.refinery_targets:
                        self.refinery_targets.append(target_geyser)
                    return actions.FUNCTIONS.Build_Refinery_screen("now", target_geyser)

            return self._select_scv(unit_type)

        # [Action 4] æŒ‡æ´¾æ¡ç“¦æ–¯ (ç¢ºä¿æœ‰ç“¦æ–¯ç”Ÿç”¢)
        elif action_id == 4:
            max_gas_allowed = refinery_count * 3
            if self.gas_workers_assigned < max_gas_allowed and self.refinery_targets:
                if actions.FUNCTIONS.Harvest_Gather_screen.id in available:
                    # æ‰¾åˆ°å·¥äººæœ€å°‘çš„ç“¦æ–¯å» ä¸¦å„ªå…ˆè£œè¶³
                    min_workers = float('inf')
                    target_refinery = None

                    scv_y, scv_x = (unit_type == SCV_ID).nonzero()
                    if scv_x.any() and scv_y.any():
                        for refinery_target in self.refinery_targets:
                            if refinery_target:
                                distances = np.sqrt((scv_x - refinery_target[0])**2 + (scv_y - refinery_target[1])**2)
                                workers_here = np.sum(distances < 10)
                                if workers_here < min_workers:
                                    min_workers = workers_here
                                    target_refinery = refinery_target

                    # å¦‚æœæ‰¾åˆ°ç›®æ¨™ç“¦æ–¯å» ï¼Œå‰‡æŒ‡æ´¾å·¥å…µ
                    if target_refinery:
                        self.gas_workers_assigned += 1
                        return actions.FUNCTIONS.Harvest_Gather_screen("now", target_refinery)
                    elif self.refinery_targets:
                        self.gas_workers_assigned += 1
                        return actions.FUNCTIONS.Harvest_Gather_screen("now", self.refinery_targets[0])

                return self._select_scv_filtered(unit_type, self.refinery_targets[0] if self.refinery_targets else None)
            return actions.FUNCTIONS.no_op()

        # [Action 5] å»ºé€ å…µç‡Ÿ (ç”Ÿç”¢æ å¥ªè€…çš„å¿…è¦å»ºç¯‰)
        elif action_id == 5:
            if not self.barracks_built and player.minerals >= 150 and actions.FUNCTIONS.Build_Barracks_screen.id in available:
                target = self._calc_barracks_pos(obs)
                return actions.FUNCTIONS.Build_Barracks_screen("now", target)
            return self._select_scv(unit_type)

        # [Action 6] ç ”ç™¼ç§‘æŠ€å¯¦é©—å®¤ (é€ æ å¥ªè€…å¿…å‚™)
        elif action_id == 6:
            if self.barracks_built and not self.techlab_built and player.minerals >= 50 and player.vespene >= 25:
                if actions.FUNCTIONS.Build_TechLab_quick.id in available:
                    return actions.FUNCTIONS.Build_TechLab_quick("now")
            return self._select_unit(unit_type, BARRACKS_ID)

        # [Action 7] è¨“ç·´æ å¥ªè€… (ä¸»è¦ç›®æ¨™ - ç”Ÿç”¢5éš»)
        elif action_id == 7:
            if (self.barracks_built and self.techlab_built and
                player.minerals >= 100 and player.vespene >= 25 and
                self.marauders_produced < 5):
                if actions.FUNCTIONS.Train_Marauder_quick.id in available:
                    self.marauders_produced += 1
                    print(f"ç”Ÿç”¢æ å¥ªè€…: {self.marauders_produced}/5")
                    if self.marauders_produced >= 5:
                        self.marauder_production_complete = True
                        print("âœ… å·²æˆåŠŸç”Ÿç”¢5éš»æ å¥ªè€…ï¼")
                    return actions.FUNCTIONS.Train_Marauder_quick("now")
            return self._select_unit(unit_type, BARRACKS_ID)

        # [Action 8] ä¸­å¿ƒæ“´æ•£æƒæ (åµå¯Ÿå‘¨é‚Š)
        elif action_id == 8:
            if self.scan_points:
                target = self.scan_points[self.current_scan_idx]
                self.current_scan_idx = (self.current_scan_idx + 1) % len(self.scan_points)
                return actions.FUNCTIONS.move_camera(target)
            return actions.FUNCTIONS.no_op()

        # [Action 9] åœ¨è¦–è§’ä¸­å¿ƒå»ºé€ äºŒç¤¦ (ç¶“æ¿Ÿæ“´å¼µ)
        elif action_id == 9:
            if player.minerals >= 400 and actions.FUNCTIONS.Build_CommandCenter_screen.id in available:
                return actions.FUNCTIONS.Build_CommandCenter_screen("now", (42, 42))
            return self._select_scv(unit_type)

        # å¦‚æœæ²’æœ‰åŒ¹é…çš„å‹•ä½œï¼Œå‰‡åŸ·è¡Œç„¡æ“ä½œ
        return actions.FUNCTIONS.no_op()

    def _update_building_status(self, unit_type):
        """æ›´æ–°å»ºç¯‰ç‰©ç‹€æ…‹"""
        barracks_pixels = np.sum(unit_type == BARRACKS_ID)
        self.barracks_built = barracks_pixels > 0

        techlab_pixels = np.sum(unit_type == BARRACKS_TECHLAB_ID)
        self.techlab_built = techlab_pixels > 0

        refinery_pixels = np.sum(unit_type == REFINERY_ID)
        self.refinery_built = refinery_pixels > 0

        marauder_pixels = np.sum(unit_type == MARAUDER_ID)
        self.marauders_produced = int(marauder_pixels / 20)

    # --- å…§éƒ¨è¼”åŠ©å‡½å¼ ---
    def _select_unit(self, unit_type, unit_id):
        y, x = (unit_type == unit_id).nonzero()
        if x.any():
            return actions.FUNCTIONS.select_point("select", (int(x.mean()), int(y.mean())))
        return actions.FUNCTIONS.no_op()

    def _select_scv(self, unit_type):
        y, x = (unit_type == SCV_ID).nonzero()
        if x.any():
            idx = random.randint(0, len(x) - 1)
            return actions.FUNCTIONS.select_point("select", (x[idx], y[idx]))
        return actions.FUNCTIONS.no_op()

    def _select_scv_filtered(self, unit_type, target):
        y, x = (unit_type == SCV_ID).nonzero()
        if x.any() and target:
            dist = np.sqrt((x - target[0])**2 + (y - target[1])**2)
            mask = dist > 15
            if mask.any():
                idx = random.choice(np.where(mask)[0])
                return actions.FUNCTIONS.select_point("select", (x[idx], y[idx]))
        return self._select_scv(unit_type)

    def _calc_depot_pos(self, unit_type):
        if self.depots_built == 0:
            target = (self.cc_x_screen + 15, self.cc_y_screen + 15)
        elif self.depots_built == 1:
            target = (self.cc_x_screen + 27, self.cc_y_screen + 15)
        else:
            target = (self.cc_x_screen + 21, self.cc_y_screen + 27)

        self.depots_built = (self.depots_built + 1) % 3
        return (np.clip(target[0], 0, 83), np.clip(target[1], 0, 83))

    def _calc_barracks_pos(self, obs):
        player_relative = obs.observation.feature_minimap[features.MINIMAP_FEATURES.player_relative.index]
        y_mini, x_mini = (player_relative == features.PlayerRelative.SELF).nonzero()
        offset_x = 30 if (x_mini.mean() if x_mini.any() else 0) < 32 else -30
        return (np.clip(42 + offset_x, 0, 83), 42)

    def _find_all_geysers(self, unit_type):
        y, x = (unit_type == GEYSER_ID).nonzero()
        geysers = []

        if x.any():
            visited = set()
            for i in range(len(x)):
                if i not in visited:
                    ax, ay = x[i], y[i]
                    mask = (np.abs(x - ax) < 10) & (np.abs(y - ay) < 10)
                    if mask.any():
                        geyser_pos = (int(x[mask].mean()), int(y[mask].mean()))
                        geysers.append(geyser_pos)
                        visited.update(np.where(mask)[0])

        return geysers

    def _assign_gas_workers_if_needed(self, obs, unit_type):
        player = obs.observation.player
        available = obs.observation.available_actions

        refinery_pixels = np.sum(unit_type == REFINERY_ID)
        refinery_count = int(refinery_pixels / 80)

        if refinery_count > 0 and self.refinery_targets:
            max_gas_allowed = refinery_count * 3

            gas_workers_actual = 0
            scv_y, scv_x = (unit_type == SCV_ID).nonzero()
            if scv_x.any() and scv_y.any():
                for refinery_target in self.refinery_targets:
                    if refinery_target:
                        distances = np.sqrt((scv_x - refinery_target[0])**2 + (scv_y - refinery_target[1])**2)
                        gas_workers_actual += np.sum(distances < 10)

            if gas_workers_actual < max_gas_allowed and actions.FUNCTIONS.Harvest_Gather_screen.id in available:
                min_workers = float('inf')
                target_refinery = None

                for refinery_target in self.refinery_targets:
                    if refinery_target:
                        distances = np.sqrt((scv_x - refinery_target[0])**2 + (scv_y - refinery_target[1])**2)
                        workers_here = np.sum(distances < 10)
                        if workers_here < min_workers:
                            min_workers = workers_here
                            target_refinery = refinery_target

                if target_refinery:
                    y, x = (unit_type == SCV_ID).nonzero()
                    if x.any() and target_refinery:
                        dist = np.sqrt((x - target_refinery[0])**2 + (y - target_refinery[1])**2)
                        mask = dist > 15
                        if mask.any():
                            valid_indices = np.where(mask)[0]
                            idx = random.choice(valid_indices)
                            return actions.FUNCTIONS.Harvest_Gather_screen("now", target_refinery)

    def _get_next_camera_position_for_geysers(self):
        camera_positions = [
            (10, 50),  # å·¦ä¸‹
            (50, 10),  # å³ä¸‹
            (10, 10),  # å·¦ä¸Š
            (50, 50),  # å³ä¸Š
            (30, 30),  # ä¸­é–“
        ]

        for pos in camera_positions:
            pos_key = f"{pos[0]}_{pos[1]}"
            if pos_key not in self.attempted_geyser_positions:
                self.attempted_geyser_positions.add(pos_key)
                return pos

        self.attempted_geyser_positions.clear()
        return camera_positions[0]

    def reset_episode(self):
        """é‡ç½®å›åˆç‹€æ…‹"""
        self.depots_built = 0
        self.refinery_targets = []
        self.gas_workers_assigned = 0
        self.base_minimap_coords = None
        self.scan_points = []
        self.current_scan_idx = 0
        self.marauders_produced = 0
        self.marauder_production_complete = False
        self.barracks_built = False
        self.techlab_built = False
        self.attempted_geyser_positions = set()
        self.current_refinery_target = None
        self.gas_worker_timer = 0

        self.current_step = 0
        self.total_reward = 0

        # é‡ç½®çå‹µç³»çµ±
        self.reward_system.reset()

        # æ¸…é™¤ç‹€æ…‹æ­·å²
        self.state_representation.clear_history()

    def end_episode(self, success=False):
        """
        çµæŸå›åˆä¸¦è¨˜éŒ„çµ±è¨ˆæ•¸æ“š

        åƒæ•¸:
        - success: æ˜¯å¦æˆåŠŸå®Œæˆç›®æ¨™
        """
        # è¨˜éŒ„å›åˆçµ±è¨ˆæ•¸æ“š
        self.data_logger.log_episode_stats(
            self.current_episode,
            self.total_reward,
            self.current_step,
            self.marauders_produced,
            success
        )

        # æ·»åŠ æ¢ç´¢ç‡åˆ°çµ±è¨ˆæ•¸æ“š
        self.data_logger.add_exploration_rate(self.q_agent.get_exploration_rate())

        # å¢åŠ å›åˆè¨ˆæ•¸
        self.current_episode += 1

        # è¨“ç·´Q-learningä»£ç†
        self.q_agent.train_from_experience(batch_size=32)

        # å¢åŠ Q-learningä»£ç†çš„å›åˆè¨ˆæ•¸
        self.q_agent.increment_episode()

        # æ·»åŠ çå‹µåˆ°Q-learningä»£ç†æ­·å²
        self.q_agent.add_reward(self.total_reward)
        self.q_agent.add_episode_length(self.current_step)
        self.q_agent.add_marauders_produced(self.marauders_produced)

        print(f"å›åˆ {self.current_episode} å®Œæˆ: çå‹µ={self.total_reward:.2f}, "
              f"æ­¥æ•¸={self.current_step}, æ å¥ªè€…={self.marauders_produced}, "
              f"æ¢ç´¢ç‡={self.q_agent.get_exploration_rate():.3f}")

    def save_model(self, filename=None):
        """
        ä¿å­˜Q-learningæ¨¡å‹

        åƒæ•¸:
        - filename: ä¿å­˜æ–‡ä»¶åï¼ˆå¯é¸ï¼‰
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"logs/AI_learning/ql_model_{timestamp}.json"

        self.q_agent.save_model(filename)
        print(f"âœ… Q-learningæ¨¡å‹å·²ä¿å­˜åˆ°: {filename}")

    def load_model(self, filename):
        """
        åŠ è¼‰Q-learningæ¨¡å‹

        åƒæ•¸:
        - filename: åŠ è¼‰æ–‡ä»¶å
        """
        self.q_agent.load_model(filename)
        print(f"âœ… Q-learningæ¨¡å‹å·²å¾ {filename} åŠ è¼‰")

    def save_training_data(self):
        """ä¿å­˜è¨“ç·´æ•¸æ“šåˆ°CSVå’ŒJSONæ–‡ä»¶"""
        self.data_logger.save_to_csv()
        self.data_logger.save_stats_to_json()

    def get_training_summary(self):
        """ç²å–è¨“ç·´æ‘˜è¦ä¿¡æ¯"""
        return self.data_logger.get_training_summary()

# =========================================================
# ğŸ® ä¸»è¨“ç·´å‡½æ•¸ - Q-learningè¨“ç·´å¾ªç’°
# =========================================================
def train_ql_agent(argv, episodes=50, max_steps=5000):
    """
    ä¸»è¨“ç·´å‡½æ•¸ï¼ŒåŸ·è¡Œå®Œæ•´çš„Q-learningè¨“ç·´å¾ªç’°

    åƒæ•¸:
    - argv: å‘½ä»¤è¡Œåƒæ•¸ï¼ˆæœªä½¿ç”¨ï¼‰
    - episodes: è¨“ç·´å›åˆæ•¸ï¼ˆé»˜èª50ï¼‰
    - max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•¸ï¼ˆé»˜èª5000ï¼‰
    """
    del argv  # åˆªé™¤æœªä½¿ç”¨çš„åƒæ•¸

    print("ğŸš€ é–‹å§‹Q-learningè¨“ç·´...")
    print(f"ç›®æ¨™: è¨“ç·´AIç”Ÿç”¢5éš»æ å¥ªè€…")
    print(f"è¨“ç·´åƒæ•¸: {episodes}å›åˆ, æ¯å›åˆæœ€å¤š{max_steps}æ­¥")

    # åˆå§‹åŒ–Q-learningç”Ÿç”¢AI
    ql_agent = QLearningProductionAI()

    # åµæ¸¬ç¾åœ¨æ˜¯Windowsé‚„æ˜¯Mac
    if platform.system() == "Windows":
        os.environ["SC2PATH"] = r"D:\StarCraft II"
    else:
        pass

    try:
        # åˆå§‹åŒ–StarCraft IIç’°å¢ƒ
        with sc2_env.SC2Env(
            map_name="Simple64",
            players=[sc2_env.Agent(sc2_env.Race.terran),
                     sc2_env.Bot(sc2_env.Race.zerg, sc2_env.Difficulty.easy)],
            agent_interface_format=sc2_env.AgentInterfaceFormat(
                feature_dimensions=sc2_env.Dimensions(screen=84, minimap=64),
                use_raw_units=False),
            step_mul=16,
            realtime=False,
        ) as env:
            # è¨“ç·´å¾ªç’°
            for episode in range(episodes):
                print(f"\nğŸ® é–‹å§‹å›åˆ {episode + 1}/{episodes}")

                # é‡ç½®ç’°å¢ƒ
                obs_list = env.reset()
                ql_agent.reset_episode()

                # å›åˆå¾ªç’°
                for step in range(max_steps):
                    # ç²å–å‹•ä½œ
                    sc2_action, action_id, reward, q_value = ql_agent.get_action(obs_list[0])

                    # åŸ·è¡Œå‹•ä½œ
                    obs_list = env.step([sc2_action])

                    # æª¢æŸ¥æ˜¯å¦å®Œæˆç›®æ¨™
                    if ql_agent.marauders_produced >= 5:
                        ql_agent.end_episode(success=True)
                        break

                    # æª¢æŸ¥æ˜¯å¦éŠæˆ²çµæŸ
                    if obs_list[0].last():
                        success = ql_agent.marauders_produced >= 5
                        ql_agent.end_episode(success=success)
                        break

                # æ¯10å›åˆä¿å­˜ä¸€æ¬¡æ¨¡å‹å’Œæ•¸æ“š
                if (episode + 1) % 10 == 0:
                    ql_agent.save_model()
                    ql_agent.save_training_data()

                    # é¡¯ç¤ºè¨“ç·´é€²åº¦
                    summary = ql_agent.get_training_summary()
                    if summary:
                        print(f"\nğŸ“Š è¨“ç·´é€²åº¦ï¼ˆ{episode + 1}å›åˆï¼‰ï¼š")
                        print(f"   å¹³å‡çå‹µ: {summary.get('avg_reward', 0):.2f}")
                        print(f"   æœ€å¤§çå‹µ: {summary.get('max_reward', 0):.2f}")
                        print(f"   æˆåŠŸç‡: {summary.get('success_rate', 0) * 100:.1f}%")
                        print(f"   ç¸½æ å¥ªè€…ç”Ÿç”¢: {summary.get('total_marauders', 0)}")
                        print(f"   å¹³å‡å›åˆé•·åº¦: {summary.get('avg_episode_length', 0):.0f}æ­¥")

            # è¨“ç·´å®Œæˆï¼Œä¿å­˜æœ€çµ‚æ¨¡å‹å’Œæ•¸æ“š
            ql_agent.save_model()
            ql_agent.save_training_data()

            # é¡¯ç¤ºæœ€çµ‚è¨“ç·´æ‘˜è¦
            final_summary = ql_agent.get_training_summary()
            print(f"\nğŸ‰ è¨“ç·´å®Œæˆï¼")
            print(f"ç¸½å›åˆæ•¸: {final_summary.get('total_episodes', 0)}")
            print(f"ç¸½æ­¥æ•¸: {final_summary.get('total_steps', 0)}")
            print(f"å¹³å‡çå‹µ: {final_summary.get('avg_reward', 0):.2f}")
            print(f"æœ€å¤§çå‹µ: {final_summary.get('max_reward', 0):.2f}")
            print(f"æˆåŠŸç‡: {final_summary.get('success_rate', 0) * 100:.1f}%")
            print(f"ç¸½æ å¥ªè€…ç”Ÿç”¢: {final_summary.get('total_marauders', 0)}")
            print(f"ç¸½è¨“ç·´æ™‚é–“: {final_summary.get('total_training_time', 0):.0f}ç§’")
            print(f"æ•¸æ“šå·²ä¿å­˜åˆ°: {final_summary.get('csv_file', 'æœªçŸ¥')}")
            print(f"çµ±è¨ˆå·²ä¿å­˜åˆ°: {final_summary.get('json_file', 'æœªçŸ¥')}")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ è¨“ç·´è¢«æ‰‹å‹•ä¸­æ–·")
        # ä¿å­˜ä¸­æ–·æ™‚çš„æ¨¡å‹å’Œæ•¸æ“š
        ql_agent.save_model()
        ql_agent.save_training_data()
        print("âœ… æ¨¡å‹å’Œæ•¸æ“šå·²ä¿å­˜")

# =========================================================
# ğŸ ä¸»ç¨‹å¼å…¥å£
# =========================================================
if __name__ == "__main__":
    # ä½¿ç”¨absl.app.runä¾†é‹è¡Œä¸»è¨“ç·´å‡½æ•¸
    app.run(train_ql_agent)
